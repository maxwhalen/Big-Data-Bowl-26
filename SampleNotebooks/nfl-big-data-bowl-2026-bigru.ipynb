{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d27fb2",
   "metadata": {
    "papermill": {
     "duration": 0.005234,
     "end_time": "2025-10-15T10:42:05.460997",
     "exception": false,
     "start_time": "2025-10-15T10:42:05.455763",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "split by StratifiedKFold\n",
    "\n",
    "Per-fold RMSEs: [0.62349, 0.63324, 0.60064, 0.67419, 0.63202]\n",
    "\n",
    "OOF CV RMSE: 0.63278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb25509b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:05.470427Z",
     "iopub.status.busy": "2025-10-15T10:42:05.470198Z",
     "iopub.status.idle": "2025-10-15T10:42:12.600587Z",
     "shell.execute_reply": "2025-10-15T10:42:12.599961Z"
    },
    "papermill": {
     "duration": 7.136433,
     "end_time": "2025-10-15T10:42:12.602012",
     "exception": false,
     "start_time": "2025-10-15T10:42:05.465579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# NFL BIG DATA BOWL 2026 - COMPLETE WORKING SOLUTION\n",
    "# Predicting player movement during pass plays with temporal features\n",
    "# ================================================================================\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "# Deep Learning\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import math\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f5634e",
   "metadata": {
    "papermill": {
     "duration": 0.00392,
     "end_time": "2025-10-15T10:42:12.610380",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.606460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e202b82f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.619587Z",
     "iopub.status.busy": "2025-10-15T10:42:12.619255Z",
     "iopub.status.idle": "2025-10-15T10:42:12.704476Z",
     "shell.execute_reply": "2025-10-15T10:42:12.703868Z"
    },
    "papermill": {
     "duration": 0.091223,
     "end_time": "2025-10-15T10:42:12.705679",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.614456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n",
    "    \n",
    "    NN_PRETRAIN_DIR = '/kaggle/input/nfl-big-data-bowl-2026-public/bigru-public'\n",
    "    SEED = 42\n",
    "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
    "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n",
    "    N_FOLDS = 5\n",
    "    USE_PLAYERS_INTERACTIONS =  True\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    WINDOW_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "974395a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.715123Z",
     "iopub.status.busy": "2025-10-15T10:42:12.714541Z",
     "iopub.status.idle": "2025-10-15T10:42:12.723615Z",
     "shell.execute_reply": "2025-10-15T10:42:12.723100Z"
    },
    "papermill": {
     "duration": 0.014622,
     "end_time": "2025-10-15T10:42:12.724619",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.709997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_global_seeds(seed: int = 42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_global_seeds(Config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b22a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.733462Z",
     "iopub.status.busy": "2025-10-15T10:42:12.733259Z",
     "iopub.status.idle": "2025-10-15T10:42:12.741264Z",
     "shell.execute_reply": "2025-10-15T10:42:12.740656Z"
    },
    "papermill": {
     "duration": 0.013804,
     "end_time": "2025-10-15T10:42:12.742303",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.728499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(debug_fraction=1.0):\n",
    "    \"\"\"Load all training and test data with an option to use a fraction for debugging.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Training data\n",
    "    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "    \n",
    "    # Filter existing files\n",
    "    train_input_files = [f for f in train_input_files if f.exists()]\n",
    "    train_output_files = [f for f in train_output_files if f.exists()]\n",
    "    \n",
    "    print(f\"Found {len(train_input_files)} weeks of data\")\n",
    "    \n",
    "    # Load and concatenate with week column\n",
    "    train_input = pd.concat(\n",
    "        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_input_files, start=1)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    train_output = pd.concat(\n",
    "        [pd.read_csv(f).assign(week=w) for w, f in enumerate(train_output_files, start=1)],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    # Test data\n",
    "    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n",
    "    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n",
    "    \n",
    "    print(f\"Loaded {len(train_input):,} input records, {len(train_output):,} output records\")\n",
    "    \n",
    "    # Use only a fraction of the games for debugging (select entire games)\n",
    "    if debug_fraction < 1.0:\n",
    "        unique_game_ids = train_input['game_id'].unique()\n",
    "        sampled_game_ids = pd.Series(unique_game_ids).sample(frac=debug_fraction, random_state=42).values\n",
    "        train_input = train_input[train_input['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n",
    "        train_output = train_output[train_output['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n",
    "        print(f\"Using {len(train_input):,} input records from {len(sampled_game_ids)} games for debugging\")\n",
    "    \n",
    "    return train_input, train_output, test_input, test_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4a969",
   "metadata": {
    "papermill": {
     "duration": 0.0038,
     "end_time": "2025-10-15T10:42:12.750224",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.746424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bae18f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.758765Z",
     "iopub.status.busy": "2025-10-15T10:42:12.758555Z",
     "iopub.status.idle": "2025-10-15T10:42:12.765799Z",
     "shell.execute_reply": "2025-10-15T10:42:12.765284Z"
    },
    "papermill": {
     "duration": 0.012702,
     "end_time": "2025-10-15T10:42:12.766747",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.754045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute RMSE for NFL competition.\n",
    "    Expected input:\n",
    "      - solution and submission as pandas.DataFrame\n",
    "      - Column 'id': unique identifier for each (game_id, play_id, nfl_id, frame_id)\n",
    "      - Column 'x'\n",
    "      - Column 'y'\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = 'id'\n",
    "    >>> solution = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,3], 'y':[4,2,3]})\n",
    "    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1.1,2,3], 'y':[4,2.2,3]})\n",
    "    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n",
    "    0.0913\n",
    "    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [0,2,3], 'y':[4,2.2,3]})\n",
    "    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n",
    "    0.4163\n",
    "    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,1], 'y':[4,0,3]})\n",
    "    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n",
    "    1.1547\n",
    "    \"\"\"\n",
    "\n",
    "    TARGET = ['x', 'y']\n",
    "    if row_id_column_name not in solution.columns:\n",
    "        raise ParticipantVisibleError(f\"Solution file missing required column: '{row_id_column_name}'\")\n",
    "    if row_id_column_name not in submission.columns:\n",
    "        raise ParticipantVisibleError(f\"Submission file missing required column: '{row_id_column_name}'\")\n",
    "\n",
    "    missing_in_solution = set(TARGET) - set(solution.columns)\n",
    "    missing_in_submission = set(TARGET) - set(submission.columns)\n",
    "\n",
    "    if missing_in_solution:\n",
    "        raise ParticipantVisibleError(f'Solution file missing required columns: {missing_in_solution}')\n",
    "    if missing_in_submission:\n",
    "        raise ParticipantVisibleError(f'Submission file missing required columns: {missing_in_submission}')\n",
    "\n",
    "    submission = submission[['id'] + TARGET]\n",
    "    merged_df = pd.merge(solution, submission, on=row_id_column_name, suffixes=('_true', '_pred'))\n",
    "    #log NaN\n",
    "    nanx_in_pred = merged_df['x_pred'].isna().sum()\n",
    "    nany_in_pred = merged_df['y_pred'].isna().sum()\n",
    "    if nanx_in_pred > 0:\n",
    "        print(f\"WARNING: Found {nanx_in_pred} NaN predictions in merged results\")\n",
    "    if nany_in_pred > 0:\n",
    "        print(f\"WARNING: Found {nany_in_pred} NaN predictions in merged results\")\n",
    "    nanx_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['x_true'].isna().sum()\n",
    "    nany_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['y_true'].isna().sum()\n",
    "    if nanx_in_true > 0:\n",
    "        print(f\"WARNING: Found {nanx_in_true} NaN true values corresponding to NaN predictions\")\n",
    "    if nany_in_true > 0:\n",
    "        print(f\"WARNING: Found {nany_in_true} NaN true values corresponding to NaN predictions\")\n",
    "    rmse = np.sqrt(\n",
    "        0.5 * (mean_squared_error(merged_df['x_true'], merged_df['x_pred']) + mean_squared_error(merged_df['y_true'], merged_df['y_pred']))\n",
    "    )\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8eff6c",
   "metadata": {
    "papermill": {
     "duration": 0.003748,
     "end_time": "2025-10-15T10:42:12.774425",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.770677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74439cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.783060Z",
     "iopub.status.busy": "2025-10-15T10:42:12.782736Z",
     "iopub.status.idle": "2025-10-15T10:42:12.798921Z",
     "shell.execute_reply": "2025-10-15T10:42:12.798420Z"
    },
    "papermill": {
     "duration": 0.021741,
     "end_time": "2025-10-15T10:42:12.799985",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.778244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "from collections import defaultdict\n",
    "\n",
    "def _compute_interactions_for_play_frames(df_play_frames: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute interaction features per (game_id, play_id, frame_id, nfl_id) for selected players:\n",
    "      - distance_to_player_mean/min/max_offense/defense\n",
    "      - relative_velocity_magnitude_mean/min/max_offense/defense\n",
    "      - angle_to_player_mean/min/max_offense/defense  (mean is circular)\n",
    "      - nearest_opponent_dist/angle/rel_speed\n",
    "\n",
    "    Only computes (emits rows) where 'player_to_predict' is True if that column exists.\n",
    "    Otherwise computes for all players.\n",
    "\n",
    "    df_play_frames contains rows for a single (game_id, play_id), multiple frame_ids and all players in those frames.\n",
    "    \"\"\"\n",
    "    out_rows = []\n",
    "    # Per frame to keep matrices tiny\n",
    "    for (g, p, f), grp in df_play_frames.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "        n = len(grp)\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        nfl_ids = grp['nfl_id'].to_numpy()\n",
    "        x  = grp['x'].to_numpy(dtype=np.float32)\n",
    "        y  = grp['y'].to_numpy(dtype=np.float32)\n",
    "        vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "        vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "        is_off = grp['is_offense'].to_numpy().astype(bool)\n",
    "\n",
    "        compute_mask = grp['player_to_predict'].to_numpy().astype(bool) if 'player_to_predict' in grp.columns else np.ones(n, dtype=bool)\n",
    "\n",
    "        # Pairwise geometry\n",
    "        dx = x[None, :] - x[:, None]\n",
    "        dy = y[None, :] - y[:, None]\n",
    "        dist = np.sqrt(dx * dx + dy * dy)                  # (n,n)\n",
    "        angle_mat = np.arctan2(-dy, -dx)                   # angle i->j (y_j - y_i, x_j - x_i)\n",
    "        dvx = vx[:, None] - vx[None, :]\n",
    "        dvy = vy[:, None] - vy[None, :]\n",
    "        rel_speed = np.sqrt(dvx * dvx + dvy * dvy)         # (n,n)\n",
    "\n",
    "        # Masks\n",
    "        opp_mask = (is_off[:, None] != is_off[None, :])    # opponent pairs\n",
    "        np.fill_diagonal(opp_mask, False)\n",
    "\n",
    "        mask_off = np.broadcast_to(is_off[None, :], (n, n)).copy()\n",
    "        mask_def = np.broadcast_to(~is_off[None, :], (n, n)).copy()\n",
    "        np.fill_diagonal(mask_off, False)\n",
    "        np.fill_diagonal(mask_def, False)\n",
    "\n",
    "        # Nearest opponent\n",
    "        dist_opp = np.where(opp_mask, dist, np.nan)\n",
    "        nearest_dist = np.nanmin(dist_opp, axis=1)\n",
    "        nearest_idx = np.nanargmin(dist_opp, axis=1)\n",
    "        all_nan = ~np.isfinite(nearest_dist)\n",
    "        nearest_idx_safe = nearest_idx.copy()\n",
    "        nearest_idx_safe[all_nan] = 0\n",
    "        nearest_angle = np.take_along_axis(angle_mat, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "        nearest_rel   = np.take_along_axis(rel_speed, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "        nearest_angle[all_nan] = np.nan\n",
    "        nearest_rel[all_nan]   = np.nan\n",
    "\n",
    "        # Group-wise aggregations\n",
    "        # Distances\n",
    "        d_off = np.where(mask_off, dist, np.nan)\n",
    "        d_def = np.where(mask_def, dist, np.nan)\n",
    "        d_mean_o = np.nanmean(d_off, axis=1); d_min_o = np.nanmin(d_off, axis=1); d_max_o = np.nanmax(d_off, axis=1)\n",
    "        d_mean_d = np.nanmean(d_def, axis=1); d_min_d = np.nanmin(d_def, axis=1); d_max_d = np.nanmax(d_def, axis=1)\n",
    "\n",
    "        # Relative speed\n",
    "        v_off = np.where(mask_off, rel_speed, np.nan)\n",
    "        v_def = np.where(mask_def, rel_speed, np.nan)\n",
    "        v_mean_o = np.nanmean(v_off, axis=1); v_min_o = np.nanmin(v_off, axis=1); v_max_o = np.nanmax(v_off, axis=1)\n",
    "        v_mean_d = np.nanmean(v_def, axis=1); v_min_d = np.nanmin(v_def, axis=1); v_max_d = np.nanmax(v_def, axis=1)\n",
    "\n",
    "        # Angles: circular mean for mean, raw min/max for spread reference\n",
    "        sinA = np.sin(angle_mat); cosA = np.cos(angle_mat)\n",
    "\n",
    "        cnt_off = mask_off.sum(axis=1).astype(np.float32)\n",
    "        cnt_def = mask_def.sum(axis=1).astype(np.float32)\n",
    "\n",
    "        # Avoid divide-by-zero; set denom=nan where no neighbors\n",
    "        denom_off = np.where(cnt_off > 0, cnt_off, np.nan)\n",
    "        denom_def = np.where(cnt_def > 0, cnt_def, np.nan)\n",
    "\n",
    "        sin_sum_off = (sinA * mask_off).sum(axis=1)\n",
    "        cos_sum_off = (cosA * mask_off).sum(axis=1)\n",
    "        sin_sum_def = (sinA * mask_def).sum(axis=1)\n",
    "        cos_sum_def = (cosA * mask_def).sum(axis=1)\n",
    "\n",
    "        a_mean_o = np.arctan2(sin_sum_off / denom_off, cos_sum_off / denom_off)\n",
    "        a_mean_d = np.arctan2(sin_sum_def / denom_def, cos_sum_def / denom_def)\n",
    "\n",
    "        a_off = np.where(mask_off, angle_mat, np.nan)\n",
    "        a_def = np.where(mask_def, angle_mat, np.nan)\n",
    "        a_min_o = np.nanmin(a_off, axis=1); a_max_o = np.nanmax(a_off, axis=1)\n",
    "        a_min_d = np.nanmin(a_def, axis=1); a_max_d = np.nanmax(a_def, axis=1)\n",
    "\n",
    "        # Emit only for players to predict\n",
    "        for idx, nid in enumerate(nfl_ids):\n",
    "            if not compute_mask[idx]:\n",
    "                continue\n",
    "            out_rows.append({\n",
    "                'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': int(nid),\n",
    "\n",
    "                'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                'distance_to_player_min_offense': d_min_o[idx],\n",
    "                'distance_to_player_max_offense': d_max_o[idx],\n",
    "                'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                'relative_velocity_magnitude_min_offense': v_min_o[idx],\n",
    "                'relative_velocity_magnitude_max_offense': v_max_o[idx],\n",
    "                'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                'angle_to_player_min_offense': a_min_o[idx],\n",
    "                'angle_to_player_max_offense': a_max_o[idx],\n",
    "\n",
    "                'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                'distance_to_player_min_defense': d_min_d[idx],\n",
    "                'distance_to_player_max_defense': d_max_d[idx],\n",
    "                'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                'relative_velocity_magnitude_min_defense': v_min_d[idx],\n",
    "                'relative_velocity_magnitude_max_defense': v_max_d[idx],\n",
    "                'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                'angle_to_player_min_defense': a_min_d[idx],\n",
    "                'angle_to_player_max_defense': a_max_d[idx],\n",
    "\n",
    "                'nearest_opponent_dist': float(nearest_dist[idx]) if np.isfinite(nearest_dist[idx]) else np.nan,\n",
    "                'nearest_opponent_angle': float(nearest_angle[idx]) if np.isfinite(nearest_angle[idx]) else np.nan,\n",
    "                'nearest_opponent_rel_speed': float(nearest_rel[idx]) if np.isfinite(nearest_rel[idx]) else np.nan,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(out_rows, columns=[\n",
    "        'game_id', 'play_id', 'frame_id', 'nfl_id',\n",
    "        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n",
    "        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n",
    "        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n",
    "        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n",
    "        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n",
    "        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n",
    "        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed'\n",
    "    ])\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9611b536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.808523Z",
     "iopub.status.busy": "2025-10-15T10:42:12.808140Z",
     "iopub.status.idle": "2025-10-15T10:42:12.811644Z",
     "shell.execute_reply": "2025-10-15T10:42:12.810959Z"
    },
    "papermill": {
     "duration": 0.008914,
     "end_time": "2025-10-15T10:42:12.812725",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.803811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def height_to_feet(height_str):\n",
    "    \"\"\"Convert height from 'ft-in' format to feet\"\"\"\n",
    "    try:\n",
    "        ft, inches = map(int, height_str.split('-'))\n",
    "        return ft + inches/12\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e79b5724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.821462Z",
     "iopub.status.busy": "2025-10-15T10:42:12.821192Z",
     "iopub.status.idle": "2025-10-15T10:42:12.832426Z",
     "shell.execute_reply": "2025-10-15T10:42:12.831881Z"
    },
    "papermill": {
     "duration": 0.016769,
     "end_time": "2025-10-15T10:42:12.833483",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.816714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_advanced_features(df):\n",
    "    \"\"\"\n",
    "    STEP 2: Add 30-40 advanced features\n",
    "    These are proven to improve performance\n",
    "    \"\"\"\n",
    "    print(\"Adding advanced features...\")\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 1: Distance Rate Features (3)\n",
    "    # ==========================================\n",
    "    if 'distance_to_ball' in df.columns:\n",
    "        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n",
    "        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n",
    "        df['time_to_intercept'] = (df['distance_to_ball'] / \n",
    "                                    (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 2: Target Alignment Features (3)\n",
    "    # ==========================================\n",
    "    if 'ball_direction_x' in df.columns:\n",
    "        df['velocity_alignment'] = (\n",
    "            df['velocity_x'] * df['ball_direction_x'] +\n",
    "            df['velocity_y'] * df['ball_direction_y']\n",
    "        )\n",
    "        df['velocity_perpendicular'] = (\n",
    "            df['velocity_x'] * (-df['ball_direction_y']) +\n",
    "            df['velocity_y'] * df['ball_direction_x']\n",
    "        )\n",
    "        if 'acceleration_x' in df.columns:\n",
    "            df['accel_alignment'] = (\n",
    "                df['acceleration_x'] * df['ball_direction_x'] +\n",
    "                df['acceleration_y'] * df['ball_direction_y']\n",
    "            )\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 3: Multi-Window Rolling (24)\n",
    "    # ==========================================\n",
    "    for window in [3, 5, 10]:\n",
    "        for col in ['velocity_x', 'velocity_y', 's', 'a']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_roll{window}'] = df.groupby(gcols)[col].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).mean()\n",
    "                )\n",
    "                df[f'{col}_std{window}'] = df.groupby(gcols)[col].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).std()\n",
    "                ).fillna(0)\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 4: Extended Lag Features (8)\n",
    "    # ==========================================\n",
    "    for lag in [4, 5]:\n",
    "        for col in ['x', 'y', 'velocity_x', 'velocity_y']:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag).fillna(0)\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 5: Velocity Change Features (4)\n",
    "    # ==========================================\n",
    "    if 'velocity_x' in df.columns:\n",
    "        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n",
    "        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n",
    "        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n",
    "        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n",
    "        df['direction_change'] = df['direction_change'].apply(\n",
    "            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n",
    "        )\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 6: Field Position Features (4)\n",
    "    # ==========================================\n",
    "    df['dist_from_left'] = df['y']\n",
    "    df['dist_from_right'] = 53.3 - df['y']\n",
    "    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n",
    "    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 7: Role-Specific Features (3)\n",
    "    # ==========================================\n",
    "    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n",
    "        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n",
    "        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n",
    "    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n",
    "        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n",
    "    \n",
    "    # ==========================================\n",
    "    # GROUP 8: Time Features (2)\n",
    "    # ==========================================\n",
    "    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n",
    "    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n",
    "        lambda x: x / (x.max() + 1)\n",
    "    )\n",
    "    \n",
    "    print(f\"Total features after enhancement: {len(df.columns)}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54f35d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.842377Z",
     "iopub.status.busy": "2025-10-15T10:42:12.842171Z",
     "iopub.status.idle": "2025-10-15T10:42:12.874490Z",
     "shell.execute_reply": "2025-10-15T10:42:12.873758Z"
    },
    "papermill": {
     "duration": 0.038104,
     "end_time": "2025-10-15T10:42:12.875503",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.837399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(input_df, output_df=None, test_template=None, \n",
    "                                            is_training=True, window_size=8,use_players_interactions=Config.USE_PLAYERS_INTERACTIONS):\n",
    "    \"\"\"\n",
    "    Prepare sequences with ALL advanced features\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Window size: {window_size}\")\n",
    "    \n",
    "    input_df = input_df.copy()\n",
    "    \n",
    "    # ==========================================\n",
    "    # BASIC FEATURES\n",
    "    # ==========================================\n",
    "    print(\"Step 1/4: Adding basic features...\")\n",
    "    \n",
    "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
    "    \n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    delta_t = 0.1\n",
    "    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "    input_df['acceleration_x'] = input_df['a'] * np.sin(dir_rad)\n",
    "    input_df['acceleration_y'] = input_df['a'] * np.cos(dir_rad)\n",
    "    input_df['o_sin'] = np.sin(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['o_cos'] = np.cos(np.deg2rad(input_df['o'].fillna(0)))\n",
    "    input_df['dir_sin'] = np.sin(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "    input_df['dir_cos'] = np.cos(np.deg2rad(input_df['dir'].fillna(0)))\n",
    "    # Roles\n",
    "    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
    "    \n",
    "    # Physics\n",
    "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n",
    "    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n",
    "    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n",
    "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "    \n",
    "    # Ball features\n",
    "    if 'ball_land_x' in input_df.columns:\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
    "        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed'] = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "    \n",
    "    # Sort for temporal\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
    "    \n",
    "    # Original lag features (1-3)\n",
    "    for lag in [1, 2, 3]:\n",
    "        input_df[f'x_lag{lag}'] = input_df.groupby(gcols)['x'].shift(lag)\n",
    "        input_df[f'y_lag{lag}'] = input_df.groupby(gcols)['y'].shift(lag)\n",
    "        input_df[f'velocity_x_lag{lag}'] = input_df.groupby(gcols)['velocity_x'].shift(lag)\n",
    "        input_df[f'velocity_y_lag{lag}'] = input_df.groupby(gcols)['velocity_y'].shift(lag)\n",
    "    \n",
    "    # EMA features\n",
    "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
    "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
    "    )\n",
    "    \n",
    "    # ==========================================\n",
    "    # ADVANCED FEATURES (NEW!)\n",
    "    # ==========================================\n",
    "    print(\"Step 2/4: Adding advanced features...\")\n",
    "    input_df = add_advanced_features(input_df)\n",
    "    # ==========================================\n",
    "    # PLAYER INTERACTION FEATURES (NEW!)\n",
    "    # ==========================================\n",
    "    print(\"Step 3/4: Adding player interaction features...\")\n",
    "    if use_players_interactions:\n",
    "        agg_rows = []\n",
    "        # Group once (avoid overhead of apply per small group)\n",
    "        for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "            n = len(grp)\n",
    "            nfl_ids = grp['nfl_id'].to_numpy()\n",
    "            # Only compute/emit for player_to_predict==True (if column exists)\n",
    "            compute_mask = grp['player_to_predict'].to_numpy().astype(bool) if 'player_to_predict' in grp.columns else np.ones(n, dtype=bool)\n",
    "            if n < 2:\n",
    "                # Create empty stats rows (NaNs) only for players to predict\n",
    "                for nid in nfl_ids[compute_mask]:\n",
    "                    agg_rows.append({\n",
    "                        'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                        'distance_to_player_mean_offense': np.nan,\n",
    "                        'distance_to_player_min_offense': np.nan,\n",
    "                        'distance_to_player_max_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_offense': np.nan,\n",
    "                        'angle_to_player_mean_offense': np.nan,\n",
    "                        'angle_to_player_min_offense': np.nan,\n",
    "                        'angle_to_player_max_offense': np.nan,\n",
    "                        'distance_to_player_mean_defense': np.nan,\n",
    "                        'distance_to_player_min_defense': np.nan,\n",
    "                        'distance_to_player_max_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_defense': np.nan,\n",
    "                        'angle_to_player_mean_defense': np.nan,\n",
    "                        'angle_to_player_min_defense': np.nan,\n",
    "                        'angle_to_player_max_defense': np.nan,\n",
    "                        'nearest_opponent_dist': np.nan,\n",
    "                        'nearest_opponent_angle': np.nan,\n",
    "                        'nearest_opponent_rel_speed': np.nan,\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            x = grp['x'].to_numpy(dtype=np.float32)\n",
    "            y = grp['y'].to_numpy(dtype=np.float32)\n",
    "            vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "            vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "            is_offense = grp['is_offense'].to_numpy()\n",
    "            is_defense = grp['is_defense'].to_numpy()\n",
    "\n",
    "            # Pairwise deltas (broadcast)\n",
    "            dx = x[None, :] - x[:, None]        # (n,n) x_j - x_i reversed later for angle\n",
    "            dy = y[None, :] - y[:, None]\n",
    "            # Angle from i -> j (want y_j - y_i, x_j - x_i)\n",
    "            angle_mat = np.arctan2(-dy, -dx)    # because dx currently x[None]-x[:,None] => -(x_j - x_i)\n",
    "\n",
    "            # Distances\n",
    "            dist = np.sqrt(dx ** 2 + dy ** 2)\n",
    "            # Relative velocity magnitudes\n",
    "            dvx = vx[:, None] - vx[None, :]\n",
    "            dvy = vy[:, None] - vy[None, :]\n",
    "            rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n",
    "\n",
    "            # Offense mask (exclude self)\n",
    "            offense_mask = (is_offense[:, None] == is_offense[None, :])\n",
    "            offense_mask = (is_offense[:, None] == is_offense[None, :])\n",
    "            np.fill_diagonal(offense_mask, False)\n",
    "\n",
    "            # Defense mask (exclude self)\n",
    "            defense_mask = (is_defense[:, None] == is_defense[None, :])\n",
    "            np.fill_diagonal(defense_mask, False)\n",
    "\n",
    "            # Opponent mask (exclude self)\n",
    "            opp_mask = (is_offense[:, None] != is_offense[None, :])\n",
    "            np.fill_diagonal(opp_mask, False)\n",
    "\n",
    "            # Mask out self distances\n",
    "            dist_diag_nan = dist.copy()\n",
    "            np.fill_diagonal(dist_diag_nan, np.nan)\n",
    "            rel_diag_nan = rel_speed.copy()\n",
    "            np.fill_diagonal(rel_diag_nan, np.nan)\n",
    "            angle_diag_nan = angle_mat.copy()\n",
    "            np.fill_diagonal(angle_diag_nan, np.nan)\n",
    "\n",
    "            def masked_stats(mat, mask):\n",
    "                # mat, mask shape (n,n)\n",
    "                masked = np.where(mask, mat, np.nan)\n",
    "                cnt = mask.sum(axis=1)\n",
    "                mean = np.nanmean(masked, axis=1)\n",
    "                amin = np.nanmin(masked, axis=1)\n",
    "                amax = np.nanmax(masked, axis=1)\n",
    "                # Rows with zero valid -> set nan\n",
    "                zero = cnt == 0\n",
    "                mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n",
    "                return mean, amin, amax\n",
    "\n",
    "            d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n",
    "            v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n",
    "            a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n",
    "\n",
    "            d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n",
    "            v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n",
    "            a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n",
    "\n",
    "            # NEW: nearest opponent stats\n",
    "            masked_dist_opp = np.where(opp_mask, dist_diag_nan, np.nan)         # (n,n)\n",
    "            nearest_dist = np.nanmin(masked_dist_opp, axis=1)                   # (n,)\n",
    "            nearest_idx = np.nanargmin(masked_dist_opp, axis=1)                 # (n,)\n",
    "            # Guard where all-NaN rows (no opponents)\n",
    "            all_nan = ~np.isfinite(nearest_dist)\n",
    "            nearest_idx_safe = nearest_idx.copy()\n",
    "            nearest_idx_safe[all_nan] = 0\n",
    "            nearest_angle = np.take_along_axis(angle_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_rel = np.take_along_axis(rel_diag_nan, nearest_idx_safe[:, None], axis=1).squeeze(1)\n",
    "            nearest_angle[all_nan] = np.nan\n",
    "            nearest_rel[all_nan] = np.nan\n",
    "\n",
    "            for idx, nid in enumerate(nfl_ids):\n",
    "                if not compute_mask[idx]:\n",
    "                    continue  # only for player_to_predict==True\n",
    "                agg_rows.append({\n",
    "                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                    'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                    'distance_to_player_min_offense': d_min_o[idx],\n",
    "                    'distance_to_player_max_offense': d_max_o[idx],\n",
    "                    'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                    'relative_velocity_magnitude_min_offense': v_min_o[idx],\n",
    "                    'relative_velocity_magnitude_max_offense': v_max_o[idx],\n",
    "                    'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                    'angle_to_player_min_offense': a_min_o[idx],\n",
    "                    'angle_to_player_max_offense': a_max_o[idx],\n",
    "                    'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                    'distance_to_player_min_defense': d_min_d[idx],\n",
    "                    'distance_to_player_max_defense': d_max_d[idx],\n",
    "                    'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                    'relative_velocity_magnitude_min_defense': v_min_d[idx],\n",
    "                    'relative_velocity_magnitude_max_defense': v_max_d[idx],\n",
    "                    'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                    'angle_to_player_min_defense': a_min_d[idx],\n",
    "                    'angle_to_player_max_defense': a_max_d[idx],\n",
    "                    'nearest_opponent_dist': nearest_dist[idx],\n",
    "                    'nearest_opponent_angle': nearest_angle[idx],\n",
    "                    'nearest_opponent_rel_speed': nearest_rel[idx],\n",
    "                })\n",
    "\n",
    "        interaction_agg = pd.DataFrame(agg_rows)\n",
    "        input_df = input_df.merge(\n",
    "            interaction_agg,\n",
    "            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Skipping fast interaction feature computation (use_fast_interactions=False).\")\n",
    "    # ==========================================\n",
    "    # FEATURE LIST (ENHANCED)\n",
    "    # ==========================================\n",
    "    print(\"Step 4/4: Creating sequences...\")\n",
    "    \n",
    "    feature_cols = [\n",
    "        # Core (6)\n",
    "        'x', 'y', 's', 'a', 'ball_land_x', 'ball_land_y',\n",
    "\n",
    "        # Angles encoded (4)\n",
    "        'o_sin', 'o_cos', 'dir_sin', 'dir_cos',\n",
    "\n",
    "        # Player (2)\n",
    "        'player_height_feet', 'player_weight',\n",
    "        \n",
    "        # Motion (6)\n",
    "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
    "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
    "        \n",
    "        # Roles (5)\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "        \n",
    "        # Ball (5)\n",
    "        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y', 'closing_speed',\n",
    "        \n",
    "        # Original temporal (15)\n",
    "        'x_lag1', 'y_lag1', 'velocity_x_lag1', 'velocity_y_lag1',\n",
    "        'x_lag2', 'y_lag2', 'velocity_x_lag2', 'velocity_y_lag2',\n",
    "        'x_lag3', 'y_lag3', 'velocity_x_lag3', 'velocity_y_lag3',\n",
    "        'velocity_x_ema', 'velocity_y_ema', 'speed_ema',\n",
    "        \n",
    "        # NEW: Distance rate (3)\n",
    "        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
    "        \n",
    "        # NEW: Target alignment (3)\n",
    "        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
    "        \n",
    "        # NEW: Multi-window rolling (24)\n",
    "        'velocity_x_roll3', 'velocity_x_std3', 'velocity_y_roll3', 'velocity_y_std3',\n",
    "        's_roll3', 's_std3', 'a_roll3', 'a_std3',\n",
    "        'velocity_x_roll5', 'velocity_x_std5', 'velocity_y_roll5', 'velocity_y_std5',\n",
    "        's_roll5', 's_std5', 'a_roll5', 'a_std5',\n",
    "        'velocity_x_roll10', 'velocity_x_std10', 'velocity_y_roll10', 'velocity_y_std10',\n",
    "        's_roll10', 's_std10', 'a_roll10', 'a_std10',\n",
    "        \n",
    "        # NEW: Extended lags (8)\n",
    "        'x_lag4', 'y_lag4', 'velocity_x_lag4', 'velocity_y_lag4',\n",
    "        'x_lag5', 'y_lag5', 'velocity_x_lag5', 'velocity_y_lag5',\n",
    "        \n",
    "        # NEW: Velocity changes (4)\n",
    "        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
    "        \n",
    "        # NEW: Field position (4)\n",
    "        'dist_from_sideline', 'dist_from_endzone',\n",
    "        \n",
    "        # NEW: Role-specific (3)\n",
    "        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
    "        \n",
    "        # NEW: Time (2)\n",
    "        'frames_elapsed', 'normalized_time',\n",
    "        # New: Player interaction (21)\n",
    "        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n",
    "        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n",
    "        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n",
    "        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n",
    "        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n",
    "        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',\n",
    "        'nearest_opponent_dist', 'nearest_opponent_angle', 'nearest_opponent_rel_speed',\n",
    "    ]\n",
    "    \n",
    "    # Filter to existing\n",
    "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
    "    print(f\"Using {len(feature_cols)}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # CREATE SEQUENCES\n",
    "    # ==========================================\n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "    \n",
    "    target_rows = output_df if is_training else test_template\n",
    "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "    \n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "    \n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "        \n",
    "        try:\n",
    "            group_df = grouped.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        input_window = group_df.tail(window_size)\n",
    "        \n",
    "        if len(input_window) < window_size:\n",
    "            # if is_training:\n",
    "            #     print(f\"Skipping sequence with insufficient history for {key}\")\n",
    "            #     continue\n",
    "            print(f\"Padding sequence with insufficient history for {key}\")\n",
    "            pad_len = window_size - len(input_window)\n",
    "            first = input_window.iloc[0:1].copy()\n",
    "            pad_df = pd.concat([first] * pad_len, ignore_index=True)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "        \n",
    "        # fill within the window only, no future mean usage\n",
    "        input_window = input_window.ffill().bfill()\n",
    "        input_window = input_window.fillna(0.0)\n",
    "\n",
    "        seq = input_window[feature_cols].values\n",
    "        if np.isnan(seq).any():\n",
    "            # print which sequence has NaNs\n",
    "            print(f\"NaNs found in sequence for {key}, replacing with 0.0\")\n",
    "            # ensure no NaNs go into the model\n",
    "            seq = np.nan_to_num(seq, nan=0.0)\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        \n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id']==row['game_id']) &\n",
    "                (output_df['play_id']==row['play_id']) &\n",
    "                (output_df['nfl_id']==row['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "            \n",
    "            last_x = input_window.iloc[-1]['x']\n",
    "            last_y = input_window.iloc[-1]['y']\n",
    "            \n",
    "            dx = out_grp['x'].values - last_x\n",
    "            dy = out_grp['y'].values - last_y\n",
    "            \n",
    "            targets_dx.append(dx)\n",
    "            targets_dy.append(dy)\n",
    "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "        \n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': input_window.iloc[-1]['frame_id']\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n",
    "    \n",
    "    if is_training:\n",
    "        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, feature_cols\n",
    "    return sequences, sequence_ids, feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "461d7e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:12.884058Z",
     "iopub.status.busy": "2025-10-15T10:42:12.883804Z",
     "iopub.status.idle": "2025-10-15T10:42:31.327273Z",
     "shell.execute_reply": "2025-10-15T10:42:31.326326Z"
    },
    "papermill": {
     "duration": 18.449041,
     "end_time": "2025-10-15T10:42:31.328481",
     "exception": false,
     "start_time": "2025-10-15T10:42:12.879440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Loading data...\n",
      "Found 18 weeks of data\n",
      "Loaded 4,880,579 input records, 562,936 output records\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preparing data...\")\n",
    "train_input, train_output, test_input, test_template = load_data(debug_fraction=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672f69b7",
   "metadata": {
    "papermill": {
     "duration": 0.00397,
     "end_time": "2025-10-15T10:42:31.336876",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.332906",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933b4993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:31.345914Z",
     "iopub.status.busy": "2025-10-15T10:42:31.345658Z",
     "iopub.status.idle": "2025-10-15T10:42:31.349960Z",
     "shell.execute_reply": "2025-10-15T10:42:31.349268Z"
    },
    "papermill": {
     "duration": 0.010063,
     "end_time": "2025-10-15T10:42:31.351090",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.341027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"Add Gaussian noise to input tensor\"\"\"\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(x) * self.stddev\n",
    "            return x + noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0ad1b2",
   "metadata": {
    "papermill": {
     "duration": 0.003923,
     "end_time": "2025-10-15T10:42:31.359080",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.355157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9917ae10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:31.368484Z",
     "iopub.status.busy": "2025-10-15T10:42:31.367938Z",
     "iopub.status.idle": "2025-10-15T10:42:31.373390Z",
     "shell.execute_reply": "2025-10-15T10:42:31.372824Z"
    },
    "papermill": {
     "duration": 0.011135,
     "end_time": "2025-10-15T10:42:31.374400",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.363265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SeqModel(nn.Module):\n",
    "    def __init__(self, input_dim, horizon):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
    "        self.pool_ln = nn.LayerNorm(256)\n",
    "        self.pool_attn = nn.MultiheadAttention(256, num_heads=4, batch_first=True)\n",
    "        self.pool_query = nn.Parameter(torch.randn(1, 1, 256))\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, horizon)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, _ = self.gru(x)\n",
    "        B = h.size(0)\n",
    "        q = self.pool_query.expand(B, -1, -1)\n",
    "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n",
    "        out = self.head(ctx.squeeze(1))\n",
    "        return torch.cumsum(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889ccc8",
   "metadata": {
    "papermill": {
     "duration": 0.003928,
     "end_time": "2025-10-15T10:42:31.382414",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.378486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d1a21ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:31.391535Z",
     "iopub.status.busy": "2025-10-15T10:42:31.391329Z",
     "iopub.status.idle": "2025-10-15T10:42:31.400716Z",
     "shell.execute_reply": "2025-10-15T10:42:31.400006Z"
    },
    "papermill": {
     "duration": 0.015162,
     "end_time": "2025-10-15T10:42:31.401788",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.386626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def build_axis_model_from_config(cfg):\n",
    "    \"\"\"\n",
    "    Instantiate the SeqModel from a saved config.\n",
    "    \"\"\"\n",
    "    input_dim = cfg['input_dim']\n",
    "    horizon = cfg['horizon']\n",
    "    return SeqModel(input_dim=input_dim, horizon=horizon)\n",
    "\n",
    "\n",
    "def _model_tag_from_instance(model):\n",
    "    if isinstance(model, SeqModel):\n",
    "        return 'seq'\n",
    "    return model.__class__.__name__.lower()\n",
    "\n",
    "\n",
    "def create_model_save_config(model, input_dim, horizon):\n",
    "    \"\"\"\n",
    "    Build a minimal config for re-instantiation of SeqModel.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'model': 'seq',\n",
    "        'input_dim': int(input_dim),\n",
    "        'horizon': int(horizon),\n",
    "    }\n",
    "\n",
    "\n",
    "def save_axis_checkpoint(model, cfg, fold_dir, axis_name='x'):\n",
    "    \"\"\"\n",
    "    Save checkpoint for SeqModel.\n",
    "    \"\"\"\n",
    "    cfg = dict(cfg or {})\n",
    "    cfg['model'] = 'seq'\n",
    "    path = Path(fold_dir) / f'axis_{axis_name}.pt'\n",
    "    torch.save({'state_dict': model.state_dict(), 'config': cfg}, str(path))\n",
    "\n",
    "\n",
    "def load_axis_checkpoint(fold_dir, axis_name='x', device=None):\n",
    "    \"\"\"\n",
    "    Load SeqModel checkpoint.\n",
    "    \"\"\"\n",
    "    device = device or Config.DEVICE\n",
    "    ckpt_path = Path(fold_dir) / f'axis_{axis_name}.pt'\n",
    "    ckpt = torch.load(str(ckpt_path), map_location=device)\n",
    "    cfg = ckpt['config']\n",
    "    state_dict = ckpt['state_dict']\n",
    "\n",
    "    try:\n",
    "        model = SeqModel(input_dim=cfg['input_dim'], horizon=cfg['horizon']).to(device)\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        model.eval()\n",
    "        return model, cfg\n",
    "    except Exception as e:\n",
    "        print(f\"[{axis_name}] SeqModel load failed ({e}); retrying with strict=False.\")\n",
    "\n",
    "    # Last resort: best-effort non-strict load\n",
    "    model = SeqModel(input_dim=cfg['input_dim'], horizon=cfg['horizon']).to(device)\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[{axis_name}] Warning: missing={len(missing)}, unexpected={len(unexpected)} when loading non-strict.\")\n",
    "    model.eval()\n",
    "    return model, cfg\n",
    "\n",
    "\n",
    "def load_folds_xy(num_folds, models_dir=None, device=None):\n",
    "    \"\"\"\n",
    "    Load SeqModel checkpoints for all folds.\n",
    "    \"\"\"\n",
    "    device = device or Config.DEVICE\n",
    "    base = Path(models_dir) if models_dir else Path('.')\n",
    "    models_x, models_y, scalers, cfgs = [], [], [], []\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        fold_dir = base / f'fold_{fold}'\n",
    "        try:\n",
    "            mx, cfgx = load_axis_checkpoint(fold_dir, 'x', device=device)\n",
    "            my, cfgy = load_axis_checkpoint(fold_dir, 'y', device=device)\n",
    "            scaler = joblib.load(str(fold_dir / 'lstm_feature_scaler_fold.joblib'))\n",
    "            models_x.append(mx)\n",
    "            models_y.append(my)\n",
    "            scalers.append(scaler)\n",
    "            cfgs.append(cfgx)\n",
    "            print(f'Loaded fold {fold} OK')\n",
    "        except Exception as e:\n",
    "            print(f'Fold {fold} load failed: {e}')\n",
    "    return models_x, models_y, scalers, cfgs\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92fadfb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:31.410758Z",
     "iopub.status.busy": "2025-10-15T10:42:31.410585Z",
     "iopub.status.idle": "2025-10-15T10:42:31.420748Z",
     "shell.execute_reply": "2025-10-15T10:42:31.420238Z"
    },
    "papermill": {
     "duration": 0.015973,
     "end_time": "2025-10-15T10:42:31.421776",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.405803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_ensemble_predictions_xy(\n",
    "    models_x, models_y, scalers, X_test_unscaled, test_seq_ids, test_template, batch_size=1024\n",
    "):\n",
    "    \"\"\"\n",
    "    Ensemble test-time predictions using separate axis models (dx and dy) across folds.\n",
    "    - models_x, models_y: lists of FlexibleSeqModel (same length, one per fold)\n",
    "    - scalers: list of StandardScaler, aligned with models (or None entries)\n",
    "    - X_test_unscaled: list/array of (T,F) sequences (unscaled)\n",
    "    - test_seq_ids: list of dicts with keys [game_id, play_id, nfl_id, frame_id(last)]\n",
    "    - test_template: DataFrame with required submission rows\n",
    "\n",
    "    Returns: DataFrame with columns [id, x, y]\n",
    "    \"\"\"\n",
    "    if len(models_x) == 0 or len(models_x) != len(models_y):\n",
    "        print(\"No axis models or mismatched model counts.\")\n",
    "        return None\n",
    "    if scalers is not None and len(scalers) != len(models_x):\n",
    "        raise ValueError(\"Length of scalers must match number of folds (or be None).\")\n",
    "\n",
    "    # Convert sequences to array of objects for robust handling\n",
    "    X_test_unscaled = np.array(X_test_unscaled, dtype=object)\n",
    "    N = len(X_test_unscaled)\n",
    "\n",
    "    # Last observed absolute positions from the sequences (assumes feat[0]=x, feat[1]=y)\n",
    "    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n",
    "    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n",
    "\n",
    "    # Per-fold cumulative displacement predictions\n",
    "    per_fold_dx = []\n",
    "    per_fold_dy = []\n",
    "\n",
    "    for i in range(len(models_x)):\n",
    "        model_x = models_x[i]\n",
    "        model_y = models_y[i]\n",
    "        scaler = scalers[i] if scalers is not None else None\n",
    "\n",
    "        # Scale per sequence for this fold\n",
    "        if scaler is not None:\n",
    "            scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n",
    "        else:\n",
    "            scaled = X_test_unscaled\n",
    "\n",
    "        # Stack to (N,T,F)\n",
    "        X = np.stack(scaled.astype(np.float32))\n",
    "        device = next(model_x.parameters()).device\n",
    "        ds = TensorDataset(torch.from_numpy(X))\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        dx_list, dy_list = [], []\n",
    "        model_x.eval(); model_y.eval()\n",
    "        with torch.no_grad():\n",
    "            for (batch,) in dl:\n",
    "                batch = batch.to(device)    # (B,T,F)\n",
    "                dx = model_x(batch)         # (B,H)\n",
    "                dy = model_y(batch)         # (B,H)\n",
    "                dx_list.append(dx.cpu().numpy())\n",
    "                dy_list.append(dy.cpu().numpy())\n",
    "        dx_cum = np.vstack(dx_list)  # (N,H)\n",
    "        dy_cum = np.vstack(dy_list)  # (N,H)\n",
    "\n",
    "        per_fold_dx.append(dx_cum)\n",
    "        per_fold_dy.append(dy_cum)\n",
    "\n",
    "    # Ensemble by mean across folds\n",
    "    ens_dx = np.mean(np.stack(per_fold_dx, axis=0), axis=0)  # (N,H)\n",
    "    ens_dy = np.mean(np.stack(per_fold_dy, axis=0), axis=0)  # (N,H)\n",
    "\n",
    "    # Create submission rows by mapping to test_template frame order per (game,play,nfl)\n",
    "    test_meta = pd.DataFrame(test_seq_ids)\n",
    "    out_rows = []\n",
    "    H = ens_dx.shape[1]\n",
    "    for i, seq_info in test_meta.iterrows():\n",
    "        game_id = int(seq_info['game_id'])\n",
    "        play_id = int(seq_info['play_id'])\n",
    "        nfl_id = int(seq_info['nfl_id'])\n",
    "\n",
    "        frame_ids = (\n",
    "            test_template[\n",
    "                (test_template['game_id'] == game_id) &\n",
    "                (test_template['play_id'] == play_id) &\n",
    "                (test_template['nfl_id'] == nfl_id)\n",
    "            ]['frame_id'].sort_values().tolist()\n",
    "        )\n",
    "        for t, frame_id in enumerate(frame_ids):\n",
    "            tt = t if t < H else H - 1\n",
    "            px = np.clip(x_last[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "            py = np.clip(y_last[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            out_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n",
    "                'x': px,\n",
    "                'y': py\n",
    "            })\n",
    "    submission = pd.DataFrame(out_rows)\n",
    "    return submission\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f171b1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:31.431233Z",
     "iopub.status.busy": "2025-10-15T10:42:31.430711Z",
     "iopub.status.idle": "2025-10-15T10:42:31.439552Z",
     "shell.execute_reply": "2025-10-15T10:42:31.438802Z"
    },
    "papermill": {
     "duration": 0.014462,
     "end_time": "2025-10-15T10:42:31.440585",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.426123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_ensemble_val_predictions(models, scalers, X_val_unscaled, val_ids, y_val_dx_fold, y_val_dy_fold, val_data, exclude_fold=None):\n",
    "    \"\"\"\n",
    "    Generate ensemble predictions for validation data and prepare for scoring.\n",
    "    Excludes the model from the same fold to prevent potential overfitting/leakage.\n",
    "    \n",
    "    Args:\n",
    "        models: List of trained models\n",
    "        scalers: List of scalers (one per model)\n",
    "        X_val_unscaled: Validation sequences (unscaled)\n",
    "        val_ids: List of dicts with sequence metadata\n",
    "        y_val_dx_fold, y_val_dy_fold: Ground truth displacements\n",
    "        val_data: DataFrame with x_last, y_last\n",
    "        exclude_fold: Index of the fold to exclude (0-based)\n",
    "    \n",
    "    Returns:\n",
    "        ensemble_pred_df, ensemble_true_df: DataFrames for scoring\n",
    "    \"\"\"\n",
    "    pred_rows = []\n",
    "    true_rows = []\n",
    "    \n",
    "    for i, seq_info in enumerate(val_ids):\n",
    "        game_id = seq_info['game_id']\n",
    "        play_id = seq_info['play_id']\n",
    "        nfl_id = seq_info['nfl_id']\n",
    "        x_last = val_data.iloc[i]['x_last']\n",
    "        y_last = val_data.iloc[i]['y_last']\n",
    "        \n",
    "        # Ground truth\n",
    "        dx_true = y_val_dx_fold[i]\n",
    "        dy_true = y_val_dy_fold[i]\n",
    "        for t in range(len(dx_true)):\n",
    "            frame_rel = t + 1\n",
    "            true_x = x_last + dx_true[t]\n",
    "            true_y = y_last + dy_true[t]\n",
    "            true_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n",
    "                'x': true_x,\n",
    "                'y': true_y\n",
    "            })\n",
    "        \n",
    "        # Ensemble predictions (exclude the model from the same fold)\n",
    "        per_model_dx = []\n",
    "        per_model_dy = []\n",
    "        for j, model in enumerate(models):\n",
    "            if exclude_fold is not None and j == exclude_fold:\n",
    "                continue  # Skip the model trained on this fold\n",
    "            scaler = scalers[j]\n",
    "            scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n",
    "            scaled_seq = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(scaled_seq).cpu().numpy()[0]  # (max_frames_output, 2)\n",
    "            per_model_dx.append(output[:, 0])\n",
    "            per_model_dy.append(output[:, 1])\n",
    "        \n",
    "        # Average across remaining models\n",
    "        if per_model_dx:  # Ensure there are models to average\n",
    "            ens_dx = np.mean(per_model_dx, axis=0)\n",
    "            ens_dy = np.mean(per_model_dy, axis=0)\n",
    "        else:\n",
    "            # Fallback: use the last known position (though this shouldn't happen with n_folds > 1)\n",
    "            ens_dx = np.zeros(len(dx_true))\n",
    "            ens_dy = np.zeros(len(dy_true))\n",
    "        \n",
    "        # Generate predictions for each frame\n",
    "        for t in range(len(dx_true)):\n",
    "            pred_x = x_last + ens_dx[t]\n",
    "            pred_y = y_last + ens_dy[t]\n",
    "            pred_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{t+1}\",\n",
    "                'x': np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n",
    "                'y': np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048c64e",
   "metadata": {
    "papermill": {
     "duration": 0.003945,
     "end_time": "2025-10-15T10:42:31.448561",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.444616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f4374d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:31.457740Z",
     "iopub.status.busy": "2025-10-15T10:42:31.457319Z",
     "iopub.status.idle": "2025-10-15T10:42:32.360160Z",
     "shell.execute_reply": "2025-10-15T10:42:32.359384Z"
    },
    "papermill": {
     "duration": 0.908529,
     "end_time": "2025-10-15T10:42:32.361254",
     "exception": false,
     "start_time": "2025-10-15T10:42:31.452725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained models from /kaggle/input/nfl-big-data-bowl-2026-public/bigru-public\n",
      "Loaded fold 1 OK\n",
      "Loaded fold 2 OK\n",
      "Loaded fold 3 OK\n",
      "Loaded fold 4 OK\n",
      "Loaded fold 5 OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Loading pretrained models from {Config.NN_PRETRAIN_DIR}\")\n",
    "models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=Config.NN_PRETRAIN_DIR, device=Config.DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11812876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:32.371392Z",
     "iopub.status.busy": "2025-10-15T10:42:32.370956Z",
     "iopub.status.idle": "2025-10-15T10:42:46.552326Z",
     "shell.execute_reply": "2025-10-15T10:42:46.551512Z"
    },
    "papermill": {
     "duration": 14.187709,
     "end_time": "2025-10-15T10:42:46.553660",
     "exception": false,
     "start_time": "2025-10-15T10:42:32.365951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPARING SEQUENCES WITH ADVANCED FEATURES\n",
      "================================================================================\n",
      "Window size: 12\n",
      "Step 1/4: Adding basic features...\n",
      "Step 2/4: Adding advanced features...\n",
      "Adding advanced features...\n",
      "Total features after enhancement: 111\n",
      "Step 3/4: Adding player interaction features...\n",
      "Step 4/4: Creating sequences...\n",
      "Using 114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating sequences: 100%|██████████| 472/472 [00:01<00:00, 358.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 472 sequences with 114 features each\n",
      "Prepared 472 test sequences with shape: (12, 114).\n"
     ]
    }
   ],
   "source": [
    "# Build test sequences\n",
    "test_sequences, test_seq_ids,feature_cols = prepare_sequences(\n",
    "    test_input, test_template=test_template, is_training=False, window_size=Config.WINDOW_SIZE\n",
    ")\n",
    "print(f\"Prepared {len(test_sequences)} test sequences with shape: {test_sequences[0].shape}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deeb3133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:46.565969Z",
     "iopub.status.busy": "2025-10-15T10:42:46.565749Z",
     "iopub.status.idle": "2025-10-15T10:42:47.844707Z",
     "shell.execute_reply": "2025-10-15T10:42:47.843974Z"
    },
    "papermill": {
     "duration": 1.28616,
     "end_time": "2025-10-15T10:42:47.845942",
     "exception": false,
     "start_time": "2025-10-15T10:42:46.559782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Use the trained per-fold axis models\n",
    "submission_xy = create_ensemble_predictions_xy(\n",
    "    models_x=models_x_nn,\n",
    "    models_y=models_y_nn,\n",
    "    scalers=scalers,\n",
    "    X_test_unscaled=test_sequences,\n",
    "    test_seq_ids=test_seq_ids,\n",
    "    test_template=test_template,\n",
    "    batch_size=1024\n",
    ")\n",
    "submission_xy.to_csv('submission.csv', index=False)\n",
    "print(\"Saved submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9492867d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T10:42:47.857768Z",
     "iopub.status.busy": "2025-10-15T10:42:47.857582Z",
     "iopub.status.idle": "2025-10-15T10:42:47.874601Z",
     "shell.execute_reply": "2025-10-15T10:42:47.873868Z"
    },
    "papermill": {
     "duration": 0.024185,
     "end_time": "2025-10-15T10:42:47.875725",
     "exception": false,
     "start_time": "2025-10-15T10:42:47.851540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024120805_74_54586_1</td>\n",
       "      <td>88.374481</td>\n",
       "      <td>34.301392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024120805_74_54586_2</td>\n",
       "      <td>88.719307</td>\n",
       "      <td>34.323635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024120805_74_54586_3</td>\n",
       "      <td>89.093002</td>\n",
       "      <td>34.391682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024120805_74_54586_4</td>\n",
       "      <td>89.507011</td>\n",
       "      <td>34.515759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024120805_74_54586_5</td>\n",
       "      <td>89.918579</td>\n",
       "      <td>34.684696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>2025010515_3902_55112_26</td>\n",
       "      <td>100.153320</td>\n",
       "      <td>27.626125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>2025010515_3902_55112_27</td>\n",
       "      <td>100.890907</td>\n",
       "      <td>27.925449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>2025010515_3902_55112_28</td>\n",
       "      <td>101.492355</td>\n",
       "      <td>28.222204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>2025010515_3902_55112_29</td>\n",
       "      <td>102.162735</td>\n",
       "      <td>28.368816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>2025010515_3902_55112_30</td>\n",
       "      <td>102.724625</td>\n",
       "      <td>28.582380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5837 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id           x          y\n",
       "0        2024120805_74_54586_1   88.374481  34.301392\n",
       "1        2024120805_74_54586_2   88.719307  34.323635\n",
       "2        2024120805_74_54586_3   89.093002  34.391682\n",
       "3        2024120805_74_54586_4   89.507011  34.515759\n",
       "4        2024120805_74_54586_5   89.918579  34.684696\n",
       "...                        ...         ...        ...\n",
       "5832  2025010515_3902_55112_26  100.153320  27.626125\n",
       "5833  2025010515_3902_55112_27  100.890907  27.925449\n",
       "5834  2025010515_3902_55112_28  101.492355  28.222204\n",
       "5835  2025010515_3902_55112_29  102.162735  28.368816\n",
       "5836  2025010515_3902_55112_30  102.724625  28.582380\n",
       "\n",
       "[5837 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_xy"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13825858,
     "sourceId": 114239,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 14093911,
     "datasetId": 8443465,
     "sourceId": 13384049,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 14102947,
     "datasetId": 8431728,
     "sourceId": 13392261,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 47.928601,
   "end_time": "2025-10-15T10:42:49.400258",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-15T10:42:01.471657",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

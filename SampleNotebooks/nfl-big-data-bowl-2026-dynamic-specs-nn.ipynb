{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a866a9af",
   "metadata": {
    "papermill": {
     "duration": 0.008774,
     "end_time": "2025-10-08T15:38:52.728711",
     "exception": false,
     "start_time": "2025-10-08T15:38:52.719937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I am back with another neural network, applying what I’ve learned from the last attempt:\n",
    "https://www.kaggle.com/code/llkh0a/nfl-big-data-bowl-2026-lstm\n",
    "\n",
    "- Huber loss is better than RMSE.\n",
    "- Predicting x and y separately is better than predicting the tuple (x, y) from the same model.\n",
    "- LSTM is good, and so is GRU.\n",
    "- Adding player interactions improves performance.\n",
    "- window_size > 8 might create some issues during submission, but handling it well can significantly help the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11804e0b",
   "metadata": {
    "papermill": {
     "duration": 0.007032,
     "end_time": "2025-10-08T15:38:52.743832",
     "exception": false,
     "start_time": "2025-10-08T15:38:52.736800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The lastest version include Catboost model idea from https://www.kaggle.com/code/hiwe0305/nfl-big-data-baseline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c6f05e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:38:52.758920Z",
     "iopub.status.busy": "2025-10-08T15:38:52.758715Z",
     "iopub.status.idle": "2025-10-08T15:39:03.692760Z",
     "shell.execute_reply": "2025-10-08T15:39:03.691961Z"
    },
    "papermill": {
     "duration": 10.943362,
     "end_time": "2025-10-08T15:39:03.694369",
     "exception": false,
     "start_time": "2025-10-08T15:38:52.751007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# NFL BIG DATA BOWL 2026 - COMPLETE WORKING SOLUTION\n",
    "# Predicting player movement during pass plays with temporal features\n",
    "# ================================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from tqdm import tqdm\n",
    "# Deep Learning\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0bdedb",
   "metadata": {
    "papermill": {
     "duration": 0.006779,
     "end_time": "2025-10-08T15:39:03.708659",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.701880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e3d9c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:03.723498Z",
     "iopub.status.busy": "2025-10-08T15:39:03.723165Z",
     "iopub.status.idle": "2025-10-08T15:39:03.729962Z",
     "shell.execute_reply": "2025-10-08T15:39:03.729455Z"
    },
    "papermill": {
     "duration": 0.015307,
     "end_time": "2025-10-08T15:39:03.730922",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.715615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASED_SPECS_ATTEMPT_1 = [\n",
    "    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 2, \"dropout\": 0.1, \"repeat\": 1},  # more GRU depth\n",
    "    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},\n",
    "    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n",
    "]\n",
    "\n",
    "BASED_SPECS_ATTEMPT_2 = [\n",
    "    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n",
    "    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 2},          # more Transformer depth\n",
    "    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n",
    "]\n",
    "\n",
    "BASED_SPECS_ATTEMPT_3 = [\n",
    "    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n",
    "    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},\n",
    "    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 2},                # more TCN depth\n",
    "]\n",
    "\n",
    "BASED_SPECS_ATTEMPT_4 = [\n",
    "    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n",
    "    {\"type\": \"transformer\", \"nhead\": 8, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},          # more attn heads (128 % 8 == 0)\n",
    "    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n",
    "]\n",
    "\n",
    "BASED_SPECS_ATTEMPT_5 = [\n",
    "    {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 2},  # more GRU blocks\n",
    "    {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},\n",
    "    {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f62ec38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:03.746671Z",
     "iopub.status.busy": "2025-10-08T15:39:03.746025Z",
     "iopub.status.idle": "2025-10-08T15:39:03.824415Z",
     "shell.execute_reply": "2025-10-08T15:39:03.823816Z"
    },
    "papermill": {
     "duration": 0.087172,
     "end_time": "2025-10-08T15:39:03.825481",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.738309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n",
    "    NN_PRETRAIN_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-public/results\"\n",
    "    PREPROCESSED_DATA_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-public/results\"\n",
    "    CATBOOST_PRETRAIN_DIR = \"/kaggle/input/nfl-big-data-bowl-2026-public/results/catboost\"\n",
    "    BLEND_WEIGHT = 0.45\n",
    "    SEED = 42\n",
    "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
    "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n",
    "    MAX_SPEED = 12.0\n",
    "    N_FOLDS = 5\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # LSTM_DATA_DIR = '/kaggle/input/prepare-lstm'\n",
    "    \n",
    "    HIDDEN_DIM = 128\n",
    "    NUM_LAYERS = 2\n",
    "    DROPOUT = 0.3\n",
    "    MAX_FUTURE_HORIZON = 94 #unchangable\n",
    "\n",
    "    PATIENCE = 30\n",
    "    EPOCHS = 200\n",
    "    DEBUG_FRACTION = 1.0\n",
    "    BATCH_SIZE = 256\n",
    "    LEARNING_RATE = 1e-3\n",
    "    # important parameters\n",
    "    #basic\n",
    "    BASED_SPECS = [\n",
    "        {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \n",
    "         \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n",
    "        {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \n",
    "         \"dropout\": 0.1, \"repeat\": 1},\n",
    "        {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \n",
    "         \"dropout\": 0.1, \"repeat\": 1},\n",
    "    ]\n",
    "    # BASED_SPECS = BASED_SPECS_ATTEMPT_1\n",
    "    # BASED_SPECS = BASED_SPECS_ATTEMPT_2\n",
    "    # BASED_SPECS = BASED_SPECS_ATTEMPT_3\n",
    "    # BASED_SPECS = BASED_SPECS_ATTEMPT_4\n",
    "    BASED_SPECS = BASED_SPECS_ATTEMPT_5\n",
    "    USE_PLAYERS_INTERACTIONS = True\n",
    "    WINDOW_SIZE = 8\n",
    "\n",
    "    # Set to low value if need to debug\n",
    "    # EPOCHS = 1\n",
    "    # DEBUG_FRACTION = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7327539",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:03.840531Z",
     "iopub.status.busy": "2025-10-08T15:39:03.840313Z",
     "iopub.status.idle": "2025-10-08T15:39:03.853598Z",
     "shell.execute_reply": "2025-10-08T15:39:03.852887Z"
    },
    "papermill": {
     "duration": 0.022028,
     "end_time": "2025-10-08T15:39:03.854748",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.832720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_global_seeds(seed: int = 42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    import random, os\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_global_seeds(Config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0bb4af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:03.869845Z",
     "iopub.status.busy": "2025-10-08T15:39:03.869648Z",
     "iopub.status.idle": "2025-10-08T15:39:03.877570Z",
     "shell.execute_reply": "2025-10-08T15:39:03.876855Z"
    },
    "papermill": {
     "duration": 0.016851,
     "end_time": "2025-10-08T15:39:03.878583",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.861732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# ================================================================================\n",
    "# DATA LOADING\n",
    "# ================================================================================\n",
    "\n",
    "def load_data(debug_fraction=1.0):\n",
    "    \"\"\"Load all training and test data with an option to use a fraction for debugging.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    # Training data\n",
    "    train_input_files = [Config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "    train_output_files = [Config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
    "    \n",
    "    # Filter existing files\n",
    "    train_input_files = [f for f in train_input_files if f.exists()]\n",
    "    train_output_files = [f for f in train_output_files if f.exists()]\n",
    "    \n",
    "    print(f\"Found {len(train_input_files)} weeks of data\")\n",
    "    \n",
    "    # Load and concatenate\n",
    "    train_input = pd.concat([pd.read_csv(f) for f in tqdm(train_input_files, desc=\"Input\")], ignore_index=True)\n",
    "    train_output = pd.concat([pd.read_csv(f) for f in tqdm(train_output_files, desc=\"Output\")], ignore_index=True)\n",
    "    \n",
    "    # Test data\n",
    "    test_input = pd.read_csv(Config.DATA_DIR / \"test_input.csv\")\n",
    "    test_template = pd.read_csv(Config.DATA_DIR / \"test.csv\")\n",
    "    \n",
    "    print(f\"Loaded {len(train_input):,} input records, {len(train_output):,} output records\")\n",
    "    \n",
    "    # Use only a fraction of the games for debugging (select entire games)\n",
    "    if debug_fraction < 1.0:\n",
    "        unique_game_ids = train_input['game_id'].unique()\n",
    "        sampled_game_ids = pd.Series(unique_game_ids).sample(frac=debug_fraction, random_state=42).values\n",
    "        train_input = train_input[train_input['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n",
    "        train_output = train_output[train_output['game_id'].isin(sampled_game_ids)].reset_index(drop=True)\n",
    "        print(f\"Using {len(train_input):,} input records from {len(sampled_game_ids)} games for debugging\")\n",
    "    \n",
    "    return train_input, train_output, test_input, test_template\n",
    "# ================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc934cb",
   "metadata": {
    "papermill": {
     "duration": 0.007092,
     "end_time": "2025-10-08T15:39:03.892644",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.885552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25526646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:03.907406Z",
     "iopub.status.busy": "2025-10-08T15:39:03.907211Z",
     "iopub.status.idle": "2025-10-08T15:39:03.914423Z",
     "shell.execute_reply": "2025-10-08T15:39:03.913733Z"
    },
    "papermill": {
     "duration": 0.015899,
     "end_time": "2025-10-08T15:39:03.915462",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.899563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute RMSE for NFL competition.\n",
    "    Expected input:\n",
    "      - solution and submission as pandas.DataFrame\n",
    "      - Column 'id': unique identifier for each (game_id, play_id, nfl_id, frame_id)\n",
    "      - Column 'x'\n",
    "      - Column 'y'\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = 'id'\n",
    "    >>> solution = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,3], 'y':[4,2,3]})\n",
    "    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1.1,2,3], 'y':[4,2.2,3]})\n",
    "    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n",
    "    0.0913\n",
    "    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [0,2,3], 'y':[4,2.2,3]})\n",
    "    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n",
    "    0.4163\n",
    "    >>> submission  = pd.DataFrame({'id': ['21_12_2_1', '21_12_2_2', '21_12_2_3'], 'x': [1,2,1], 'y':[4,0,3]})\n",
    "    >>> round(score(solution, submission, row_id_column_name=row_id_column_name), 4)\n",
    "    1.1547\n",
    "    \"\"\"\n",
    "\n",
    "    TARGET = ['x', 'y']\n",
    "    if row_id_column_name not in solution.columns:\n",
    "        raise ParticipantVisibleError(f\"Solution file missing required column: '{row_id_column_name}'\")\n",
    "    if row_id_column_name not in submission.columns:\n",
    "        raise ParticipantVisibleError(f\"Submission file missing required column: '{row_id_column_name}'\")\n",
    "\n",
    "    missing_in_solution = set(TARGET) - set(solution.columns)\n",
    "    missing_in_submission = set(TARGET) - set(submission.columns)\n",
    "\n",
    "    if missing_in_solution:\n",
    "        raise ParticipantVisibleError(f'Solution file missing required columns: {missing_in_solution}')\n",
    "    if missing_in_submission:\n",
    "        raise ParticipantVisibleError(f'Submission file missing required columns: {missing_in_submission}')\n",
    "\n",
    "    submission = submission[['id'] + TARGET]\n",
    "    merged_df = pd.merge(solution, submission, on=row_id_column_name, suffixes=('_true', '_pred'))\n",
    "    #log NaN\n",
    "    nanx_in_pred = merged_df['x_pred'].isna().sum()\n",
    "    nany_in_pred = merged_df['y_pred'].isna().sum()\n",
    "    if nanx_in_pred > 0:\n",
    "        print(f\"WARNING: Found {nanx_in_pred} NaN predictions in merged results\")\n",
    "    if nany_in_pred > 0:\n",
    "        print(f\"WARNING: Found {nany_in_pred} NaN predictions in merged results\")\n",
    "    nanx_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['x_true'].isna().sum()\n",
    "    nany_in_true = merged_df[merged_df['x_pred'].isna() | merged_df['y_pred'].isna()]['y_true'].isna().sum()\n",
    "    if nanx_in_true > 0:\n",
    "        print(f\"WARNING: Found {nanx_in_true} NaN true values corresponding to NaN predictions\")\n",
    "    if nany_in_true > 0:\n",
    "        print(f\"WARNING: Found {nany_in_true} NaN true values corresponding to NaN predictions\")\n",
    "    rmse = np.sqrt(\n",
    "        0.5 * (mean_squared_error(merged_df['x_true'], merged_df['x_pred']) + mean_squared_error(merged_df['y_true'], merged_df['y_pred']))\n",
    "    )\n",
    "    return float(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5829486",
   "metadata": {
    "papermill": {
     "duration": 0.006988,
     "end_time": "2025-10-08T15:39:03.929581",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.922593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare features for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c238fdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:03.944361Z",
     "iopub.status.busy": "2025-10-08T15:39:03.944152Z",
     "iopub.status.idle": "2025-10-08T15:39:03.947538Z",
     "shell.execute_reply": "2025-10-08T15:39:03.947014Z"
    },
    "papermill": {
     "duration": 0.011978,
     "end_time": "2025-10-08T15:39:03.948504",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.936526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def height_to_feet(height_str):\n",
    "    \"\"\"Convert height from 'ft-in' format to feet\"\"\"\n",
    "    try:\n",
    "        ft, inches = map(int, height_str.split('-'))\n",
    "        return ft + inches/12\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b28e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:03.964063Z",
     "iopub.status.busy": "2025-10-08T15:39:03.963845Z",
     "iopub.status.idle": "2025-10-08T15:39:03.990970Z",
     "shell.execute_reply": "2025-10-08T15:39:03.990402Z"
    },
    "papermill": {
     "duration": 0.036188,
     "end_time": "2025-10-08T15:39:03.992035",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.955847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(input_df, output_df=None, test_template=None, is_training=True,\n",
    "                               window_size=Config.WINDOW_SIZE, cache_dir=\"cache\", save_to_disk=True,\n",
    "                               use_players_interactions=Config.USE_PLAYERS_INTERACTIONS):\n",
    "    \"\"\"Prepare sequences (FAST interaction features using vectorized per-frame computation).\"\"\"\n",
    "    print(\"Preparing sequences for LSTM...\")\n",
    "    print('Using window size = ', window_size)\n",
    "    input_df = input_df.copy()\n",
    "\n",
    "    input_df['player_height_feet'] = input_df['player_height'].map(height_to_feet)\n",
    "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
    "    delta_t = 0.1\n",
    "    input_df['velocity_x'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.sin(dir_rad)\n",
    "    input_df['velocity_y'] = (input_df['s'] + 0.5 * input_df['a'] * delta_t) * np.cos(dir_rad)\n",
    "\n",
    "    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
    "    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
    "    input_df['is_receiver'] = (input_df['player_role'] == 'Receiver').astype(int)\n",
    "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
    "    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
    "\n",
    "    mass_kg = input_df['player_weight'].fillna(200.0) / 2.20462\n",
    "    input_df['momentum_x'] = input_df['velocity_x'] * mass_kg\n",
    "    input_df['momentum_y'] = input_df['velocity_y'] * mass_kg\n",
    "\n",
    "    current_date = datetime.now()\n",
    "    input_df['age'] = input_df['player_birth_date'].apply(\n",
    "        lambda x: (current_date - datetime.strptime(x, '%Y-%m-%d')).days // 365 if pd.notnull(x) else None\n",
    "    )\n",
    "    input_df['kinetic_energy'] = 0.5 * mass_kg * (input_df['s'] ** 2)\n",
    "    input_df['force'] = mass_kg * input_df['a']\n",
    "\n",
    "    input_df['rolling_mean_velocity_x'] = (\n",
    "        input_df.groupby(['game_id', 'play_id', 'nfl_id'])['velocity_x']\n",
    "        .transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "    )\n",
    "    input_df['rolling_std_acceleration'] = (\n",
    "        input_df.groupby(['game_id', 'play_id', 'nfl_id'])['a']\n",
    "        .transform(lambda x: x.rolling(window=window_size, min_periods=1).std())\n",
    "    )\n",
    "    # New features\n",
    "    input_df[\"heading_x\"] = np.sin(dir_rad)\n",
    "    input_df[\"heading_y\"] = np.cos(dir_rad)\n",
    "    input_df[\"acceleration_x\"] = input_df[\"a\"] * input_df[\"heading_x\"]\n",
    "    input_df[\"acceleration_y\"] = input_df[\"a\"] * input_df[\"heading_y\"]\n",
    "    input_df[\"accel_magnitude\"] = np.sqrt(input_df[\"acceleration_x\"]**2 + input_df[\"acceleration_y\"]**2)\n",
    "    if all(col in input_df.columns for col in ['ball_land_x', 'ball_land_y']):\n",
    "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
    "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
    "        input_df['distance_to_ball'] = np.sqrt(ball_dx ** 2 + ball_dy ** 2)\n",
    "        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
    "        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
    "        input_df['closing_speed'] = (\n",
    "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
    "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
    "        )\n",
    "        input_df['estimated_time_to_ball'] = input_df['distance_to_ball'] / 20.0\n",
    "        input_df['projected_time_to_ball'] = input_df['distance_to_ball'] / (np.abs(input_df['closing_speed']) + 0.1)\n",
    "\n",
    "    input_df['is_right'] = (input_df['play_direction'] == 'right').astype(int)\n",
    "    input_df['is_left'] = (input_df['play_direction'] == 'left').astype(int)\n",
    "    print(\"Calculating interaction features...\")\n",
    "    # -------- PLAYER INTERACTION FEATURES --------\n",
    "    if use_players_interactions:\n",
    "        agg_rows = []\n",
    "        # Group once (avoid overhead of apply per small group)\n",
    "        for (g, p, f), grp in input_df.groupby(['game_id', 'play_id', 'frame_id'], sort=False):\n",
    "            n = len(grp)\n",
    "            nfl_ids = grp['nfl_id'].to_numpy()\n",
    "            if n < 2:\n",
    "                # Create empty stats rows (NaNs) so merge still works\n",
    "                for nid in nfl_ids:\n",
    "                    agg_rows.append({\n",
    "                        'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                        'distance_to_player_mean_offense': np.nan,\n",
    "                        'distance_to_player_min_offense': np.nan,\n",
    "                        'distance_to_player_max_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_offense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_offense': np.nan,\n",
    "                        'angle_to_player_mean_offense': np.nan,\n",
    "                        'angle_to_player_min_offense': np.nan,\n",
    "                        'angle_to_player_max_offense': np.nan,\n",
    "                        'distance_to_player_mean_defense': np.nan,\n",
    "                        'distance_to_player_min_defense': np.nan,\n",
    "                        'distance_to_player_max_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_mean_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_min_defense': np.nan,\n",
    "                        'relative_velocity_magnitude_max_defense': np.nan,\n",
    "                        'angle_to_player_mean_defense': np.nan,\n",
    "                        'angle_to_player_min_defense': np.nan,\n",
    "                        'angle_to_player_max_defense': np.nan,\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            x = grp['x'].to_numpy(dtype=np.float32)\n",
    "            y = grp['y'].to_numpy(dtype=np.float32)\n",
    "            vx = grp['velocity_x'].to_numpy(dtype=np.float32)\n",
    "            vy = grp['velocity_y'].to_numpy(dtype=np.float32)\n",
    "            is_offense = grp['is_offense'].to_numpy()\n",
    "            is_defense = grp['is_defense'].to_numpy()\n",
    "\n",
    "            # Pairwise deltas (broadcast)\n",
    "            dx = x[None, :] - x[:, None]        # (n,n) x_j - x_i reversed later for angle\n",
    "            dy = y[None, :] - y[:, None]\n",
    "            # Angle from i -> j (want y_j - y_i, x_j - x_i)\n",
    "            angle_mat = np.arctan2(-dy, -dx)    # because dx currently x[None]-x[:,None] => -(x_j - x_i)\n",
    "\n",
    "            # Distances\n",
    "            dist = np.sqrt(dx ** 2 + dy ** 2)\n",
    "            # Relative velocity magnitudes\n",
    "            dvx = vx[:, None] - vx[None, :]\n",
    "            dvy = vy[:, None] - vy[None, :]\n",
    "            rel_speed = np.sqrt(dvx ** 2 + dvy ** 2)\n",
    "\n",
    "            # Offense mask (exclude self)\n",
    "            offense_mask = (is_offense[:, None] == is_offense[None, :])\n",
    "            np.fill_diagonal(offense_mask, False)\n",
    "\n",
    "            # Defense mask (exclude self)\n",
    "            defense_mask = (is_defense[:, None] == is_defense[None, :])\n",
    "            np.fill_diagonal(defense_mask, False)\n",
    "\n",
    "            # Mask out self distances\n",
    "            dist_diag_nan = dist.copy()\n",
    "            np.fill_diagonal(dist_diag_nan, np.nan)\n",
    "            rel_diag_nan = rel_speed.copy()\n",
    "            np.fill_diagonal(rel_diag_nan, np.nan)\n",
    "            angle_diag_nan = angle_mat.copy()\n",
    "            np.fill_diagonal(angle_diag_nan, np.nan)\n",
    "\n",
    "            def masked_stats(mat, mask):\n",
    "                # mat, mask shape (n,n)\n",
    "                masked = np.where(mask, mat, np.nan)\n",
    "                cnt = mask.sum(axis=1)\n",
    "                mean = np.nanmean(masked, axis=1)\n",
    "                amin = np.nanmin(masked, axis=1)\n",
    "                amax = np.nanmax(masked, axis=1)\n",
    "                # Rows with zero valid -> set nan\n",
    "                zero = cnt == 0\n",
    "                mean[zero] = np.nan; amin[zero] = np.nan; amax[zero] = np.nan\n",
    "                return mean, amin, amax\n",
    "\n",
    "            d_mean_o, d_min_o, d_max_o = masked_stats(dist_diag_nan, offense_mask)\n",
    "            v_mean_o, v_min_o, v_max_o = masked_stats(rel_diag_nan, offense_mask)\n",
    "            a_mean_o, a_min_o, a_max_o = masked_stats(angle_diag_nan, offense_mask)\n",
    "\n",
    "            d_mean_d, d_min_d, d_max_d = masked_stats(dist_diag_nan, defense_mask)\n",
    "            v_mean_d, v_min_d, v_max_d = masked_stats(rel_diag_nan, defense_mask)\n",
    "            a_mean_d, a_min_d, a_max_d = masked_stats(angle_diag_nan, defense_mask)\n",
    "\n",
    "            for idx, nid in enumerate(nfl_ids):\n",
    "                agg_rows.append({\n",
    "                    'game_id': g, 'play_id': p, 'frame_id': f, 'nfl_id': nid,\n",
    "                    'distance_to_player_mean_offense': d_mean_o[idx],\n",
    "                    'distance_to_player_min_offense': d_min_o[idx],\n",
    "                    'distance_to_player_max_offense': d_max_o[idx],\n",
    "                    'relative_velocity_magnitude_mean_offense': v_mean_o[idx],\n",
    "                    'relative_velocity_magnitude_min_offense': v_min_o[idx],\n",
    "                    'relative_velocity_magnitude_max_offense': v_max_o[idx],\n",
    "                    'angle_to_player_mean_offense': a_mean_o[idx],\n",
    "                    'angle_to_player_min_offense': a_min_o[idx],\n",
    "                    'angle_to_player_max_offense': a_max_o[idx],\n",
    "                    'distance_to_player_mean_defense': d_mean_d[idx],\n",
    "                    'distance_to_player_min_defense': d_min_d[idx],\n",
    "                    'distance_to_player_max_defense': d_max_d[idx],\n",
    "                    'relative_velocity_magnitude_mean_defense': v_mean_d[idx],\n",
    "                    'relative_velocity_magnitude_min_defense': v_min_d[idx],\n",
    "                    'relative_velocity_magnitude_max_defense': v_max_d[idx],\n",
    "                    'angle_to_player_mean_defense': a_mean_d[idx],\n",
    "                    'angle_to_player_min_defense': a_min_d[idx],\n",
    "                    'angle_to_player_max_defense': a_max_d[idx],\n",
    "                })\n",
    "\n",
    "        interaction_agg = pd.DataFrame(agg_rows)\n",
    "        input_df = input_df.merge(\n",
    "            interaction_agg,\n",
    "            on=['game_id', 'play_id', 'frame_id', 'nfl_id'],\n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Skipping fast interaction feature computation (use_fast_interactions=False).\")\n",
    "\n",
    "    # -------- (rest of original sequence creation unchanged) --------\n",
    "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
    "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
    "\n",
    "    target_rows = output_df if is_training else test_template\n",
    "    grouped_input = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
    "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
    "\n",
    "    feature_cols = [\n",
    "        # Basic player features\n",
    "        'x', 'y', 's', 'a', 'o', 'dir','frame_id','ball_land_x','ball_land_y',\n",
    "        'absolute_yardline_number',\n",
    "        'player_height_feet', 'player_weight',\n",
    "        'velocity_x', 'velocity_y',\n",
    "        'momentum_x', 'momentum_y',\n",
    "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
    "        'age', 'kinetic_energy', 'force',\n",
    "        'rolling_mean_velocity_x', 'rolling_std_acceleration',\n",
    "        # New features\n",
    "        'heading_x', 'heading_y', 'acceleration_x', 'acceleration_y', 'accel_magnitude',\n",
    "\n",
    "        \n",
    "        # Ball-related features (if available)\n",
    "        'distance_to_ball', 'angle_to_ball', 'ball_direction_x', 'ball_direction_y',\n",
    "        'closing_speed', 'estimated_time_to_ball', 'projected_time_to_ball'\n",
    "    ]\n",
    "    # Interaction features\n",
    "    players_interaction_features = [        \n",
    "        'distance_to_player_mean_offense', 'distance_to_player_min_offense', 'distance_to_player_max_offense',\n",
    "        'relative_velocity_magnitude_mean_offense', 'relative_velocity_magnitude_min_offense', 'relative_velocity_magnitude_max_offense',\n",
    "        'angle_to_player_mean_offense', 'angle_to_player_min_offense', 'angle_to_player_max_offense',\n",
    "        'distance_to_player_mean_defense', 'distance_to_player_min_defense', 'distance_to_player_max_defense',\n",
    "        'relative_velocity_magnitude_mean_defense', 'relative_velocity_magnitude_min_defense', 'relative_velocity_magnitude_max_defense',\n",
    "        'angle_to_player_mean_defense', 'angle_to_player_min_defense', 'angle_to_player_max_defense',]\n",
    "    if 'distance_to_ball' in input_df.columns:\n",
    "        feature_cols += [\n",
    "            'distance_to_ball','angle_to_ball','ball_direction_x','ball_direction_y',\n",
    "            'closing_speed','estimated_time_to_ball','projected_time_to_ball'\n",
    "        ]\n",
    "    if use_players_interactions:\n",
    "        feature_cols += players_interaction_features\n",
    "    # # remove features with too many NaNs\n",
    "    # valid_frac = input_df[feature_cols].notna().mean()\n",
    "    # #print removed features\n",
    "    # removed_features = valid_frac[valid_frac < 0.7].index.tolist()\n",
    "    # if removed_features:\n",
    "    #     print(f\"Removing {len(removed_features)} features with >30% NaNs: {removed_features}\")\n",
    "    # feature_cols = valid_frac[valid_frac >= 0.7].index.tolist()\n",
    "\n",
    "    print(f\"Using {len(feature_cols)} features for LSTM input\")\n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
    "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups)):\n",
    "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
    "        try:\n",
    "            group_df = grouped_input.get_group(key)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        input_window = group_df.tail(window_size)\n",
    "        if len(input_window) < window_size:\n",
    "            print(f\"Warning: sequence too short for {key}, got {len(input_window)} frames, needed {window_size}\")\n",
    "            if is_training:\n",
    "                continue\n",
    "            pad_len = window_size - len(input_window)\n",
    "            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
    "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
    "        input_window = input_window.fillna(group_df.mean(numeric_only=True)) #\n",
    "        seq = input_window[feature_cols].values\n",
    "        if np.isnan(seq.astype(np.float32)).any():\n",
    "            if is_training:\n",
    "                continue\n",
    "            else:\n",
    "                seq = np.nan_to_num(seq, nan=0.0)\n",
    "        sequences.append(seq)\n",
    "        last_frame_id = input_window['frame_id'].iloc[-1]\n",
    "        if is_training:\n",
    "            out_grp = output_df[\n",
    "                (output_df['game_id']==row['game_id']) &\n",
    "                (output_df['play_id']==row['play_id']) &\n",
    "                (output_df['nfl_id']==row['nfl_id'])\n",
    "            ].sort_values('frame_id')\n",
    "            last_x = input_window.iloc[-1]['x']\n",
    "            last_y = input_window.iloc[-1]['y']\n",
    "            dx = out_grp['x'].values - last_x\n",
    "            dy = out_grp['y'].values - last_y\n",
    "            targets_dx.append(dx)\n",
    "            targets_dy.append(dy)\n",
    "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
    "        sequence_ids.append({\n",
    "            'game_id': key[0],\n",
    "            'play_id': key[1],\n",
    "            'nfl_id': key[2],\n",
    "            'frame_id': last_frame_id\n",
    "        })\n",
    "    if is_training:\n",
    "        return sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids\n",
    "    return sequences, sequence_ids\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415e2d0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:04.007058Z",
     "iopub.status.busy": "2025-10-08T15:39:04.006858Z",
     "iopub.status.idle": "2025-10-08T15:39:23.360556Z",
     "shell.execute_reply": "2025-10-08T15:39:23.359613Z"
    },
    "papermill": {
     "duration": 19.3627,
     "end_time": "2025-10-08T15:39:23.361802",
     "exception": false,
     "start_time": "2025-10-08T15:39:03.999102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "Loading data...\n",
      "Found 18 weeks of data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input: 100%|██████████| 18/18 [00:17<00:00,  1.01it/s]\n",
      "Output: 100%|██████████| 18/18 [00:00<00:00, 34.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4,880,579 input records, 562,936 output records\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading and preparing data...\")\n",
    "train_input, train_output, test_input, test_template = load_data(debug_fraction=Config.DEBUG_FRACTION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7db6c711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:23.380018Z",
     "iopub.status.busy": "2025-10-08T15:39:23.379792Z",
     "iopub.status.idle": "2025-10-08T15:39:31.451218Z",
     "shell.execute_reply": "2025-10-08T15:39:31.450294Z"
    },
    "papermill": {
     "duration": 8.081595,
     "end_time": "2025-10-08T15:39:31.452359",
     "exception": false,
     "start_time": "2025-10-08T15:39:23.370764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed LSTM data from disk...\n",
      "Loaded 46036 sequences from /kaggle/input/nfl-big-data-bowl-2026-public/results\n"
     ]
    }
   ],
   "source": [
    "if Config.PREPROCESSED_DATA_DIR is not None:\n",
    "    print(\"Loading preprocessed LSTM data from disk...\")\n",
    "    lstm_data = joblib.load(os.path.join(Config.PREPROCESSED_DATA_DIR, 'lstm_sequences_targets_ids.joblib'))\n",
    "    sequences = lstm_data['sequences']\n",
    "    targets_dx = lstm_data['targets_dx']\n",
    "    targets_dy = lstm_data['targets_dy']\n",
    "    targets_frame_ids = lstm_data['targets_frame_ids']\n",
    "    sequence_ids = lstm_data['ids']\n",
    "    print(f\"Loaded {len(sequences)} sequences from {Config.PREPROCESSED_DATA_DIR}\")\n",
    "else:\n",
    "    sequences, targets_dx, targets_dy,targets_frame_ids,ids = prepare_sequences(\n",
    "        input_df=train_input,\n",
    "        output_df=train_output,\n",
    "        is_training=True,\n",
    "        window_size=Config.WINDOW_SIZE,\n",
    "    )\n",
    "    # save to /kaggle/working\n",
    "    joblib.dump({\n",
    "        'sequences': sequences,\n",
    "        'targets_dx': targets_dx,\n",
    "        'targets_dy': targets_dy,\n",
    "        'targets_frame_ids': targets_frame_ids,\n",
    "        'ids': ids\n",
    "    }, 'lstm_sequences_targets_ids.joblib')\n",
    "\n",
    "    print(\"Saved sequences, targets_dx, targets_dy, targets_frame_ids, ids to lstm_sequences_targets_ids.joblib\")\n",
    "\n",
    "# Prepare 3D sequences for LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e7fca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.470057Z",
     "iopub.status.busy": "2025-10-08T15:39:31.469832Z",
     "iopub.status.idle": "2025-10-08T15:39:31.475241Z",
     "shell.execute_reply": "2025-10-08T15:39:31.474666Z"
    },
    "papermill": {
     "duration": 0.015219,
     "end_time": "2025-10-08T15:39:31.476254",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.461035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46036, (8, 63), 46036, (21,), (21,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences),sequences[0].shape,len(targets_dx),targets_dx[0].shape,targets_dy[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a7207d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.493948Z",
     "iopub.status.busy": "2025-10-08T15:39:31.493738Z",
     "iopub.status.idle": "2025-10-08T15:39:31.500423Z",
     "shell.execute_reply": "2025-10-08T15:39:31.499871Z"
    },
    "papermill": {
     "duration": 0.016712,
     "end_time": "2025-10-08T15:39:31.501451",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.484739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_oof_predictions(model, scaler, X_val_unscaled, val_ids, y_val_dx, y_val_dy, y_val_frame_ids, val_data):\n",
    "    \"\"\"\n",
    "    Build per-frame OOF predictions using ALL models (no exclusion).\n",
    "    Returns pred_df, true_df with real frame_ids.\n",
    "    \"\"\"\n",
    "    pred_rows, true_rows = [], []\n",
    "    for i, seq_info in enumerate(val_ids):\n",
    "        game_id = seq_info['game_id']\n",
    "        play_id = seq_info['play_id']\n",
    "        nfl_id = seq_info['nfl_id']\n",
    "        x_last = val_data.iloc[i]['x_last']\n",
    "        y_last = val_data.iloc[i]['y_last']\n",
    "        dx_true = y_val_dx[i]\n",
    "        dy_true = y_val_dy[i]\n",
    "        frame_ids_future = y_val_frame_ids[i]  # real future frame_ids\n",
    "        # True rows\n",
    "        for t in range(len(dx_true)):\n",
    "            true_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_ids_future[t]}\",\n",
    "                'x': x_last + dx_true[t],\n",
    "                'y': y_last + dy_true[t]\n",
    "            })\n",
    "        # Ensemble predictions\n",
    "        per_model_dx, per_model_dy = [], []\n",
    "        \n",
    "            \n",
    "        scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n",
    "        inp = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(inp).cpu().numpy()[0]  # (H,2) cumulative dx,dy\n",
    "        per_model_dx.append(out[:,0])\n",
    "        per_model_dy.append(out[:,1])\n",
    "        ens_dx = np.mean(per_model_dx, axis=0)\n",
    "        ens_dy = np.mean(per_model_dy, axis=0)\n",
    "        # Use only required length\n",
    "        for t in range(len(dx_true)):\n",
    "            pred_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_ids_future[t]}\",\n",
    "                'x': np.clip(x_last + ens_dx[t], Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n",
    "                'y': np.clip(y_last + ens_dy[t], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX),\n",
    "            })\n",
    "    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff63ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.518737Z",
     "iopub.status.busy": "2025-10-08T15:39:31.518569Z",
     "iopub.status.idle": "2025-10-08T15:39:31.524414Z",
     "shell.execute_reply": "2025-10-08T15:39:31.523900Z"
    },
    "papermill": {
     "duration": 0.015649,
     "end_time": "2025-10-08T15:39:31.525401",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.509752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# PREDICTION UTILITIES\n",
    "# ================================================================================\n",
    "\n",
    "def displacement_to_position(displacement_dx, displacement_dy, x_last, y_last):\n",
    "    \"\"\"\n",
    "    Convert displacement predictions to absolute positions.\n",
    "    \n",
    "    Args:\n",
    "        displacement_dx: Predicted displacement in x direction\n",
    "        displacement_dy: Predicted displacement in y direction  \n",
    "        x_last: Last known x position\n",
    "        y_last: Last known y position\n",
    "        \n",
    "    Returns:\n",
    "        pred_x, pred_y: Absolute predicted positions\n",
    "    \"\"\"\n",
    "    pred_x = x_last + displacement_dx\n",
    "    pred_y = y_last + displacement_dy\n",
    "    \n",
    "    # Apply field constraints\n",
    "    pred_x = np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "    pred_y = np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "    \n",
    "    return pred_x, pred_y\n",
    "\n",
    "\n",
    "def predict_with_lstm(model, X_test, test_data):\n",
    "    \"\"\"\n",
    "    Make predictions with trained LSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        X_test: Test sequences (batch, sequence_length, features)\n",
    "        test_data: Test dataframe for position conversion\n",
    "        \n",
    "    Returns:\n",
    "        pred_x, pred_y: Absolute predicted positions\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    predictions_dx = []\n",
    "    predictions_dy = []\n",
    "    \n",
    "    # Predict in batches\n",
    "    batch_size = 1024\n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            predictions_dx.extend(outputs[:, 0].cpu().numpy())\n",
    "            predictions_dy.extend(outputs[:, 1].cpu().numpy())\n",
    "    \n",
    "    # Convert to absolute positions\n",
    "    pred_x, pred_y = displacement_to_position(\n",
    "        np.array(predictions_dx), \n",
    "        np.array(predictions_dy),\n",
    "        test_data['x_last'].values,\n",
    "        test_data['y_last'].values\n",
    "    )\n",
    "    \n",
    "    return pred_x, pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75c426",
   "metadata": {
    "papermill": {
     "duration": 0.008118,
     "end_time": "2025-10-08T15:39:31.541796",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.533678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f531d4e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.559299Z",
     "iopub.status.busy": "2025-10-08T15:39:31.559119Z",
     "iopub.status.idle": "2025-10-08T15:39:31.569381Z",
     "shell.execute_reply": "2025-10-08T15:39:31.568787Z"
    },
    "papermill": {
     "duration": 0.020199,
     "end_time": "2025-10-08T15:39:31.570406",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.550207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_test_predictions_lstm(models, X_test, test_seq_ids, test_input):\n",
    "    \"\"\"\n",
    "    Make predictions on test data using ensemble of trained LSTM models.\n",
    "    \n",
    "    Args:\n",
    "        models: List of trained LSTM models\n",
    "        X_test: Test sequences (batch, sequence_length, features)\n",
    "        test_seq_ids: Mapping info for test sequences\n",
    "        test_input: Original test input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        submission: DataFrame with id, x, y columns\n",
    "    \"\"\"\n",
    "    print(\"Making test predictions...\")\n",
    "    \n",
    "    if len(X_test) == 0:\n",
    "        print(\"WARNING: No test sequences provided. Using fallback predictions.\")\n",
    "        # Fallback: use last known positions\n",
    "        submission = pd.DataFrame({\n",
    "            'id': (test_input['game_id'].astype(str) + '_' + \n",
    "                  test_input['play_id'].astype(str) + '_' + \n",
    "                  test_input['nfl_id'].astype(str) + '_' + \n",
    "                  test_input['frame_id'].astype(str)),\n",
    "            'x': test_input['x'].values,\n",
    "            'y': test_input['y'].values\n",
    "        })\n",
    "        return submission\n",
    "    \n",
    "    print(f\"Test sequences shape: {X_test.shape}\")\n",
    "    \n",
    "    # Get ensemble predictions\n",
    "    all_predictions_dx = []\n",
    "    all_predictions_dy = []\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"Predicting with model {i+1}/{len(models)}...\")\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        model.eval()\n",
    "        \n",
    "        predictions_dx = []\n",
    "        predictions_dy = []\n",
    "        \n",
    "        # Predict in batches\n",
    "        batch_size = 512\n",
    "        test_dataset = TensorDataset(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, in test_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                \n",
    "                predictions_dx.extend(outputs[:, 0].cpu().numpy())\n",
    "                predictions_dy.extend(outputs[:, 1].cpu().numpy())\n",
    "        \n",
    "        all_predictions_dx.append(np.array(predictions_dx))\n",
    "        all_predictions_dy.append(np.array(predictions_dy))\n",
    "    \n",
    "    # Ensemble average\n",
    "    ensemble_dx = np.mean(all_predictions_dx, axis=0)\n",
    "    ensemble_dy = np.mean(all_predictions_dy, axis=0)\n",
    "    \n",
    "    # Initialize output arrays with NaN\n",
    "    final_pred_x = np.full(len(test_input), np.nan)\n",
    "    final_pred_y = np.full(len(test_input), np.nan)\n",
    "    \n",
    "    # Map predictions back to original test rows\n",
    "    for i, seq_info in enumerate(test_seq_ids):\n",
    "        # Find corresponding row in test_input\n",
    "        mask = ((test_input['game_id'] == seq_info['game_id']) &\n",
    "               (test_input['play_id'] == seq_info['play_id']) &\n",
    "               (test_input['nfl_id'] == seq_info['nfl_id']) &\n",
    "               (test_input['frame_id'] == seq_info['frame_id']))\n",
    "        \n",
    "        if mask.any():\n",
    "            # Get reference position\n",
    "            ref_x = test_input.loc[mask, 'x'].iloc[0]\n",
    "            ref_y = test_input.loc[mask, 'y'].iloc[0]\n",
    "            \n",
    "            # Convert displacement to absolute position\n",
    "            pred_x = ref_x + ensemble_dx[i]\n",
    "            pred_y = ref_y + ensemble_dy[i]\n",
    "            \n",
    "            # Store predictions\n",
    "            final_pred_x[mask] = pred_x\n",
    "            final_pred_y[mask] = pred_y\n",
    "    \n",
    "    # Fill any remaining NaN with original positions\n",
    "    nan_mask = np.isnan(final_pred_x)\n",
    "    final_pred_x[nan_mask] = test_input.loc[nan_mask, 'x'].values\n",
    "    final_pred_y[nan_mask] = test_input.loc[nan_mask, 'y'].values\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': (test_input['game_id'].astype(str) + '_' + \n",
    "              test_input['play_id'].astype(str) + '_' + \n",
    "              test_input['nfl_id'].astype(str) + '_' + \n",
    "              test_input['frame_id'].astype(str)),\n",
    "        'x': final_pred_x,\n",
    "        'y': final_pred_y\n",
    "    })\n",
    "    \n",
    "    # Final validation\n",
    "    submission['x'] = np.clip(submission['x'], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "    submission['y'] = np.clip(submission['y'], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "    \n",
    "    print(f\"Created submission with {len(submission)} predictions\")\n",
    "    print(f\"X range: [{submission['x'].min():.2f}, {submission['x'].max():.2f}]\")\n",
    "    print(f\"Y range: [{submission['y'].min():.2f}, {submission['y'].max():.2f}]\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c0d41",
   "metadata": {
    "papermill": {
     "duration": 0.008026,
     "end_time": "2025-10-08T15:39:31.586725",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.578699",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TemporalHuber1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffd927e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.604005Z",
     "iopub.status.busy": "2025-10-08T15:39:31.603618Z",
     "iopub.status.idle": "2025-10-08T15:39:31.609064Z",
     "shell.execute_reply": "2025-10-08T15:39:31.608334Z"
    },
    "papermill": {
     "duration": 0.015294,
     "end_time": "2025-10-08T15:39:31.610208",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.594914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import math\n",
    "\n",
    "class TemporalHuber1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D Temporal Huber with optional exponential time-decay.\n",
    "    pred/target: (B, L); mask: (B, L) with 1 for valid steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, delta=0.5, time_decay=0.03):\n",
    "        super().__init__()\n",
    "        self.delta = float(delta)\n",
    "        self.time_decay = float(time_decay)\n",
    "\n",
    "    def forward(self, pred, target, mask):\n",
    "        # pred, target, mask -> (B, L)\n",
    "        err = pred - target\n",
    "        abs_e = torch.abs(err)\n",
    "        per_elem = torch.where(\n",
    "            abs_e <= self.delta,\n",
    "            0.5 * err * err,\n",
    "            self.delta * (abs_e - 0.5 * self.delta)\n",
    "        )\n",
    "        if self.time_decay > 0:\n",
    "            L = pred.size(1)\n",
    "            t = torch.arange(L, device=pred.device).float()\n",
    "            w = torch.exp(-self.time_decay * t).view(1, L)\n",
    "            per_elem = per_elem * w\n",
    "            mask = mask * w\n",
    "        per_elem = per_elem * mask\n",
    "        denom = mask.sum() + 1e-8\n",
    "        return per_elem.sum() / denom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3f679",
   "metadata": {
    "papermill": {
     "duration": 0.008084,
     "end_time": "2025-10-08T15:39:31.626570",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.618486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7633db8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.644143Z",
     "iopub.status.busy": "2025-10-08T15:39:31.643619Z",
     "iopub.status.idle": "2025-10-08T15:39:31.655101Z",
     "shell.execute_reply": "2025-10-08T15:39:31.654414Z"
    },
    "papermill": {
     "duration": 0.021213,
     "end_time": "2025-10-08T15:39:31.656119",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.634906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------- Building blocks ---------------------\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, mod, dim_in, dim_out, drop_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.mod = mod\n",
    "        self.proj = nn.Identity() if dim_in == dim_out else nn.Linear(dim_in, dim_out)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D_in)\n",
    "        y = self.mod(x)                               # (B, T, D_out)\n",
    "        x_proj = self.proj(x)                         # (B, T, D_out)\n",
    "        return self.dropout(y) + x_proj\n",
    "\n",
    "\n",
    "class RNNBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, rnn=\"gru\", num_layers=1, dropout=0.1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        rnn_cls = nn.GRU if rnn.lower() == \"gru\" else nn.LSTM\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        y, _ = self.rnn(x)\n",
    "        return y  # (B, T, out_dim)\n",
    "\n",
    "\n",
    "class Conv1DBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Conv (TCN-style): depthwise separable convs with dilation, LayerNorm (on feature dim), GELU, residual.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, kernel_size=3, dilation=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        pad = (kernel_size - 1) * dilation // 2\n",
    "        self.pre_ln = nn.LayerNorm(dim)                 # apply on (B,T,D) before transpose\n",
    "        self.dw = nn.Conv1d(dim, dim, kernel_size, padding=pad, dilation=dilation, groups=dim)\n",
    "        self.pw = nn.Conv1d(dim, dim, 1)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        y = self.pre_ln(x)              # (B,T,D)\n",
    "        y = y.transpose(1, 2)           # (B,D,T)\n",
    "        y = self.dw(y)                  # (B,D,T)\n",
    "        y = self.act(y)\n",
    "        y = self.pw(y)                  # (B,D,T)\n",
    "        y = self.drop(y)\n",
    "        return y.transpose(1, 2)        # (B,T,D)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    PreNorm Transformer encoder block with MultiheadAttention + FFN + residuals.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, nhead=4, ff_mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads=nhead, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, ff_mult * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_mult * dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
    "        # x: (B, T, D)\n",
    "        h = self.ln1(x)\n",
    "        y, _ = self.attn(h, h, h, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
    "        x = x + y\n",
    "        h = self.ln2(x)\n",
    "        h = x + self.ff(h)\n",
    "        return h  # (B, T, D)\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation over features per time step.\"\"\"\n",
    "    def __init__(self, dim, r=4):\n",
    "        super().__init__()\n",
    "        hidden = max(1, dim // r)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        s = x.mean(dim=1)          # (B, D)\n",
    "        g = self.net(s).unsqueeze(1)  # (B,1,D)\n",
    "        return x * g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15f50ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.674708Z",
     "iopub.status.busy": "2025-10-08T15:39:31.674242Z",
     "iopub.status.idle": "2025-10-08T15:39:31.677895Z",
     "shell.execute_reply": "2025-10-08T15:39:31.677229Z"
    },
    "papermill": {
     "duration": 0.014639,
     "end_time": "2025-10-08T15:39:31.678972",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.664333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlockWrapper(nn.Module):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1668763c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.696336Z",
     "iopub.status.busy": "2025-10-08T15:39:31.696149Z",
     "iopub.status.idle": "2025-10-08T15:39:31.706546Z",
     "shell.execute_reply": "2025-10-08T15:39:31.705858Z"
    },
    "papermill": {
     "duration": 0.020408,
     "end_time": "2025-10-08T15:39:31.707637",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.687229",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------- Flexible model ---------------------\n",
    "\n",
    "class FlexibleSeqModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible sequence model that stacks customizable blocks and predicts a single-axis cumulative displacement\n",
    "    over H horizons. Use two instances for dx and dy (separate models), as per your better LB finding.\n",
    "    Supported blocks in block_specs:\n",
    "      {\"type\": \"rnn\", \"rnn\": \"gru\"|\"lstm\", \"hidden\": 128, \"layers\": 1, \"bidirectional\": False}\n",
    "      {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 1}\n",
    "      {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4}\n",
    "      {\"type\": \"se\"}  # squeeze-excitation\n",
    "    pooling: \"last\" | \"mean\" | \"attn\"\n",
    "    predict_mode: \"steps\" (per-step increments, then cumsum) | \"cumulative\"\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        horizon: int,\n",
    "        block_specs: list,\n",
    "        dropout: float = 0.2,\n",
    "        pooling: str = \"attn\",\n",
    "        predict_mode: str = \"steps\",\n",
    "        attn_pool_heads: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.predict_mode = predict_mode\n",
    "        self.pooling = pooling\n",
    "\n",
    "        dim = input_dim\n",
    "        blocks = []\n",
    "        for spec in block_specs:\n",
    "            t = spec[\"type\"].lower()\n",
    "            if t == \"rnn\":\n",
    "                blk = RNNBlock(\n",
    "                    input_dim=dim,\n",
    "                    hidden_dim=spec.get(\"hidden\", 128),\n",
    "                    rnn=spec.get(\"rnn\", \"gru\"),\n",
    "                    num_layers=spec.get(\"layers\", 1),\n",
    "                    dropout=spec.get(\"dropout\", 0.1),\n",
    "                    bidirectional=spec.get(\"bidirectional\", False),\n",
    "                )\n",
    "                blocks.append(Residual(blk, dim, blk.out_dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n",
    "                dim = blk.out_dim\n",
    "            elif t == \"tcn\":\n",
    "                blk = Conv1DBlock(dim, kernel_size=spec.get(\"kernel\", 3), dilation=spec.get(\"dilation\", 1), dropout=spec.get(\"dropout\", 0.1))\n",
    "                blocks.append(Residual(blk, dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n",
    "            elif t == \"transformer\":\n",
    "                blk = TransformerBlock(dim, nhead=spec.get(\"nhead\", 4), ff_mult=spec.get(\"ff_mult\", 4), dropout=spec.get(\"dropout\", 0.1))\n",
    "                blocks.append(Residual(TransformerBlockWrapper(blk), dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n",
    "            elif t == \"se\":\n",
    "                blk = SEBlock(dim, r=spec.get(\"r\", 4))\n",
    "                blocks.append(Residual(blk, dim, dim, drop_prob=spec.get(\"res_dropout\", 0.0)))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown block type: {t}\")\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        # Attention pooling (if selected)\n",
    "        if pooling == \"attn\":\n",
    "            self.pool_ln = nn.LayerNorm(dim)\n",
    "            self.pool_attn = nn.MultiheadAttention(dim, num_heads=attn_pool_heads, batch_first=True)\n",
    "            self.pool_vec = nn.Parameter(torch.randn(1, 1, dim))  # learned query token\n",
    "        elif pooling == \"mean\":\n",
    "            self.pool_ln = nn.LayerNorm(dim)\n",
    "        else:\n",
    "            self.pool_ln = nn.LayerNorm(dim)\n",
    "\n",
    "        # Head predicts either steps or cumulative for a single axis\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(dim, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, horizon)   # output: (B, H) for one axis\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, F)\n",
    "        h = x\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h)  # (B, T, D)\n",
    "\n",
    "        # pooling to (B, D)\n",
    "        if self.pooling == \"attn\":\n",
    "            B, T, D = h.shape\n",
    "            q = self.pool_vec.expand(B, -1, -1)       # (B, 1, D)\n",
    "            k = v = self.pool_ln(h)\n",
    "            ctx, _ = self.pool_attn(q, k, v)          # (B, 1, D)\n",
    "            ctx = ctx.squeeze(1)\n",
    "        elif self.pooling == \"mean\":\n",
    "            ctx = self.pool_ln(h).mean(dim=1)\n",
    "        else:\n",
    "            ctx = self.pool_ln(h[:, -1, :])  # last step\n",
    "\n",
    "        out = self.head(ctx)                   # (B, H)\n",
    "        if self.predict_mode == \"steps\":\n",
    "            out = torch.cumsum(out, dim=1)     # convert steps -> cumulative\n",
    "        return out  # (B, H) single axis cumulative\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edb7060",
   "metadata": {
    "papermill": {
     "duration": 0.008057,
     "end_time": "2025-10-08T15:39:31.723925",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.715868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cebcfb",
   "metadata": {
    "papermill": {
     "duration": 0.008118,
     "end_time": "2025-10-08T15:39:31.740280",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.732162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bfbe68c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.758392Z",
     "iopub.status.busy": "2025-10-08T15:39:31.758206Z",
     "iopub.status.idle": "2025-10-08T15:39:31.890587Z",
     "shell.execute_reply": "2025-10-08T15:39:31.889821Z"
    },
    "papermill": {
     "duration": 0.143238,
     "end_time": "2025-10-08T15:39:31.891848",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.748610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "def _prepare_targets_axis(batch_axis, max_h):\n",
    "    \"\"\"\n",
    "    Pad 1D axis targets to (B, L) and produce masks (B, L).\n",
    "    \"\"\"\n",
    "    tensors, masks, lengths = [], [], []\n",
    "    for arr in batch_axis:\n",
    "        L = len(arr)\n",
    "        pad_len = max_h - L\n",
    "        padded = np.pad(arr, (0, pad_len), constant_values=0).astype(np.float32)\n",
    "        mask = np.zeros(max_h, dtype=np.float32)\n",
    "        mask[:L] = 1.0\n",
    "        tensors.append(torch.tensor(padded, dtype=torch.float32))\n",
    "        masks.append(torch.tensor(mask, dtype=torch.float32))\n",
    "        lengths.append(L)\n",
    "    return torch.stack(tensors), torch.stack(masks), lengths\n",
    "\n",
    "\n",
    "def train_axis_model(\n",
    "    X_train, y_train_axis, X_val, y_val_axis, input_dim, horizon,\n",
    "    block_specs, pooling=\"attn\", predict_mode=\"steps\",\n",
    "    batch_size=256, epochs=100, lr=1e-3, patience=15,\n",
    "    delta=0.5, time_decay=0.03, verbose_every=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a single-axis model (dx or dy) predicting cumulative displacement over horizon.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = FlexibleSeqModel(\n",
    "        input_dim=input_dim, horizon=horizon, block_specs=block_specs,\n",
    "        pooling=pooling, predict_mode=predict_mode, dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    crit = TemporalHuber1D(delta=delta, time_decay=time_decay)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=5, verbose=False)\n",
    "\n",
    "    # Pre-batch to keep parity with your pipeline\n",
    "    max_h = horizon\n",
    "    train_batches = []\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        end = min(i + batch_size, len(X_train))\n",
    "        batch_X = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))  # (B,T,F)\n",
    "        batch_y, batch_m, lengths = _prepare_targets_axis([y_train_axis[j] for j in range(i, end)], max_h)\n",
    "        train_batches.append((batch_X, batch_y, batch_m, lengths))\n",
    "\n",
    "    val_batches = []\n",
    "    for i in range(0, len(X_val), batch_size):\n",
    "        end = min(i + batch_size, len(X_val))\n",
    "        batch_X = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n",
    "        batch_y, batch_m, lengths = _prepare_targets_axis([y_val_axis[j] for j in range(i, end)], max_h)\n",
    "        val_batches.append((batch_X, batch_y, batch_m, lengths))\n",
    "\n",
    "    best_loss, best_state, bad = float('inf'), None, 0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        tl = []\n",
    "        for bx, by, bm, _ in train_batches:\n",
    "            bx = bx.to(device); by = by.to(device); bm = bm.to(device)\n",
    "            pred = model(bx)              # (B, H)\n",
    "            loss = crit(pred, by, bm)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            tl.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        vl = []\n",
    "        with torch.no_grad():\n",
    "            for bx, by, bm, _ in val_batches:\n",
    "                bx = bx.to(device); by = by.to(device); bm = bm.to(device)\n",
    "                pred = model(bx)\n",
    "                loss = crit(pred, by, bm)\n",
    "                vl.append(loss.item())\n",
    "        v = float(np.mean(vl)) if vl else float('inf')\n",
    "        sch.step(v)\n",
    "\n",
    "        if ep % max(1, verbose_every) == 0:\n",
    "            print(f\"Axis train epoch {ep}: train {np.mean(tl):.4f} val {v:.4f}\")\n",
    "\n",
    "        if v + 1e-6 < best_loss:\n",
    "            best_loss = v\n",
    "            best_state = {k: v_.detach().cpu().clone() for k, v_ in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stop axis at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_loss\n",
    "\n",
    "\n",
    "def create_oof_predictions_xy(\n",
    "    model_x, model_y, scaler, X_val_unscaled, val_ids, y_val_dx, y_val_dy, y_val_frame_ids, val_data, horizon\n",
    "):\n",
    "    \"\"\"\n",
    "    Build OOF predictions combining separate x and y axis models.\n",
    "    Uses real future frame_ids for scoring alignment.\n",
    "    \"\"\"\n",
    "    device = next(model_x.parameters()).device\n",
    "    pred_rows, true_rows = [], []\n",
    "    for i, seq_info in enumerate(val_ids):\n",
    "        game_id = seq_info['game_id']; play_id = seq_info['play_id']; nfl_id = seq_info['nfl_id']\n",
    "        x_last = val_data.iloc[i]['x_last']; y_last = val_data.iloc[i]['y_last']\n",
    "        dx_true = y_val_dx[i]; dy_true = y_val_dy[i]\n",
    "        future_ids = y_val_frame_ids[i]\n",
    "\n",
    "        # Truth rows\n",
    "        for t in range(len(dx_true)):\n",
    "            true_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{future_ids[t]}\",\n",
    "                'x': x_last + dx_true[t],\n",
    "                'y': y_last + dy_true[t],\n",
    "            })\n",
    "\n",
    "        # Predict cumulative dx, dy\n",
    "        seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n",
    "        inp = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred_dx = model_x(inp).cpu().numpy()[0]  # (H,)\n",
    "            pred_dy = model_y(inp).cpu().numpy()[0]\n",
    "        for t in range(len(dx_true)):\n",
    "            px = np.clip(x_last + pred_dx[t], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "            py = np.clip(y_last + pred_dy[t], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            pred_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{future_ids[t]}\",\n",
    "                'x': px, 'y': py\n",
    "            })\n",
    "    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)\n",
    "\n",
    "\n",
    "def run_multi_fold_training_xy(\n",
    "    sequences, targets_dx, targets_dy, targets_frame_ids, ids,\n",
    "    block_specs,\n",
    "    pooling=\"attn\", predict_mode=\"steps\",\n",
    "    lr=1e-3, n_folds=5, epochs=100, patience=15\n",
    "):\n",
    "    # Ensure object arrays\n",
    "    if not isinstance(sequences, np.ndarray): sequences = np.array(sequences, dtype=object)\n",
    "    if not isinstance(targets_dx, np.ndarray): targets_dx = np.array(targets_dx, dtype=object)\n",
    "    if not isinstance(targets_dy, np.ndarray): targets_dy = np.array(targets_dy, dtype=object)\n",
    "    if not isinstance(targets_frame_ids, np.ndarray): targets_frame_ids = np.array(targets_frame_ids, dtype=object)\n",
    "\n",
    "    groups = np.array([d['game_id'] for d in ids])\n",
    "    gkf = GroupKFold(n_splits=n_folds)\n",
    "    input_dim = sequences[0].shape[-1]\n",
    "    H = Config.MAX_FUTURE_HORIZON\n",
    "\n",
    "    models_x, models_y, scalers = [], [], []\n",
    "    fold_metrics = []\n",
    "    oof_pred_parts, oof_true_parts = [], []\n",
    "\n",
    "    for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), start=1):\n",
    "        print(f\"\\n--- Fold {fold}/{n_folds} ---\")\n",
    "        X_tr_u = sequences[tr]; X_va_u = sequences[va]\n",
    "        dx_tr = targets_dx[tr]; dy_tr = targets_dy[tr]\n",
    "        dx_va = targets_dx[va]; dy_va = targets_dy[va]\n",
    "        fid_va = targets_frame_ids[va]\n",
    "        val_ids = [ids[i] for i in va]\n",
    "\n",
    "        # Scaler on train frames only\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(np.vstack([s for s in X_tr_u]))\n",
    "        def apply_scaler(arr): return np.array([scaler.transform(s) for s in arr], dtype=object)\n",
    "        X_tr = np.stack(apply_scaler(X_tr_u).astype(np.float32))\n",
    "        X_va = np.stack(apply_scaler(X_va_u).astype(np.float32))\n",
    "\n",
    "        # Train axis models\n",
    "        model_x, _ = train_axis_model(\n",
    "            X_tr, dx_tr, X_va, dx_va, input_dim=input_dim, horizon=H,\n",
    "            block_specs=block_specs, pooling=pooling, predict_mode=predict_mode,\n",
    "            batch_size=Config.BATCH_SIZE, epochs=epochs, lr=lr, patience=patience,\n",
    "            delta=0.5, time_decay=0.03, verbose_every=5\n",
    "        )\n",
    "        model_y, _ = train_axis_model(\n",
    "            X_tr, dy_tr, X_va, dy_va, input_dim=input_dim, horizon=H,\n",
    "            block_specs=block_specs, pooling=pooling, predict_mode=predict_mode,\n",
    "            batch_size=Config.BATCH_SIZE, epochs=epochs, lr=lr, patience=patience,\n",
    "            delta=0.5, time_decay=0.03, verbose_every=5\n",
    "        )\n",
    "\n",
    "        # Save fold models/scaler\n",
    "        models_x.append(model_x); models_y.append(model_y); scalers.append(scaler)\n",
    "        os.makedirs(f'fold_{fold}', exist_ok=True)\n",
    "        torch.save({'state_dict': model_x.state_dict(), 'config': {'input_dim': input_dim, 'horizon': H}}, f'fold_{fold}/axis_x.pt')\n",
    "        torch.save({'state_dict': model_y.state_dict(), 'config': {'input_dim': input_dim, 'horizon': H}}, f'fold_{fold}/axis_y.pt')\n",
    "        joblib.dump(scaler, f'fold_{fold}/lstm_feature_scaler_fold.joblib')\n",
    "\n",
    "        # OOF for this fold\n",
    "        val_df = pd.DataFrame(val_ids)\n",
    "        val_df['x_last'] = np.array([s[-1,0] for s in X_va_u])\n",
    "        val_df['y_last'] = np.array([s[-1,1] for s in X_va_u])\n",
    "        oof_pred, oof_true = create_oof_predictions_xy(\n",
    "            model_x, model_y, scaler, X_va_u, val_ids, dx_va, dy_va, fid_va, val_df, horizon=H\n",
    "        )\n",
    "        oof_pred_parts.append(oof_pred); oof_true_parts.append(oof_true)\n",
    "\n",
    "        # Fold score\n",
    "        fold_rmse = score(oof_true, oof_pred, 'id')\n",
    "        fold_metrics.append(fold_rmse)\n",
    "        print(f\"Fold {fold} RMSE: {fold_rmse:.5f}\")\n",
    "\n",
    "    oof_pred_df = pd.concat(oof_pred_parts, ignore_index=True).drop_duplicates('id')\n",
    "    oof_true_df = pd.concat(oof_true_parts, ignore_index=True).drop_duplicates('id')\n",
    "    cv = score(oof_true_df, oof_pred_df, 'id')\n",
    "    print(\"\\n--- Multi-Fold Summary ---\")\n",
    "    for i, m in enumerate(fold_metrics, 1):\n",
    "        print(f\"Fold {i}: {m:.5f}\")\n",
    "    print(f\"OOF CV Score: {cv:.5f}\")\n",
    "    return models_x, models_y, scalers, fold_metrics, cv, oof_pred_df\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd548237",
   "metadata": {
    "papermill": {
     "duration": 0.008297,
     "end_time": "2025-10-08T15:39:31.908975",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.900678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train 1 fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "606ae106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.926610Z",
     "iopub.status.busy": "2025-10-08T15:39:31.926165Z",
     "iopub.status.idle": "2025-10-08T15:39:31.935811Z",
     "shell.execute_reply": "2025-10-08T15:39:31.935103Z"
    },
    "papermill": {
     "duration": 0.019544,
     "end_time": "2025-10-08T15:39:31.936828",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.917284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: 46036\n",
      "First sequence shape: (8, 63)\n",
      "Targets_dx: 46036 sequences, lengths: [21, 21, 21, 9, 9]...\n",
      "Targets_dy: 46036 sequences, lengths: [21, 21, 21, 9, 9]...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train 1 fold using GroupKFold\n",
    "\n",
    "\n",
    "print(f\"Sequences shape: {len(sequences)}\")  # Already an object array\n",
    "print(f\"First sequence shape: {sequences[0].shape if len(sequences) > 0 else 'N/A'}\")\n",
    "print(f\"Targets_dx: {len(targets_dx)} sequences, lengths: {[len(dx) for dx in targets_dx[:5]]}...\")  # Show first 5 lengths\n",
    "print(f\"Targets_dy: {len(targets_dy)} sequences, lengths: {[len(dy) for dy in targets_dy[:5]]}...\")\n",
    "\n",
    "\n",
    "# Get number of output frames from the targets\n",
    "num_frames_output = [targets_dx[i].shape for i in range(len(targets_dx))]\n",
    "# print(f\"Number of output frames to predict: {num_frames_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f024b1dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:31.954379Z",
     "iopub.status.busy": "2025-10-08T15:39:31.954198Z",
     "iopub.status.idle": "2025-10-08T15:39:32.652890Z",
     "shell.execute_reply": "2025-10-08T15:39:32.652116Z"
    },
    "papermill": {
     "duration": 0.709149,
     "end_time": "2025-10-08T15:39:32.654353",
     "exception": false,
     "start_time": "2025-10-08T15:39:31.945204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "\n",
    "# Train only the first fold grouped by game_id\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure arrays are object arrays for flexible slicing\n",
    "sequences = np.array(sequences, dtype=object)\n",
    "targets_dx = np.array(targets_dx, dtype=object)\n",
    "targets_dy = np.array(targets_dy, dtype=object)\n",
    "targets_frame_ids = np.array(targets_frame_ids, dtype=object)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46c569a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:32.674513Z",
     "iopub.status.busy": "2025-10-08T15:39:32.674241Z",
     "iopub.status.idle": "2025-10-08T15:39:32.678672Z",
     "shell.execute_reply": "2025-10-08T15:39:32.677986Z"
    },
    "papermill": {
     "duration": 0.016408,
     "end_time": "2025-10-08T15:39:32.679755",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.663347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Group by game_id and take the first split\n",
    "# groups = np.array([d['game_id'] for d in ids])\n",
    "# gkf = GroupKFold(n_splits=Config.N_FOLDS)\n",
    "# train_idx, val_idx = next(gkf.split(sequences, groups=groups))\n",
    "\n",
    "# X_train_unscaled = sequences[train_idx]\n",
    "# X_val_unscaled = sequences[val_idx]\n",
    "# y_train_dx_fold = targets_dx[train_idx]\n",
    "# y_train_dy_fold = targets_dy[train_idx]\n",
    "# y_val_dx_fold = targets_dx[val_idx]\n",
    "# y_val_dy_fold = targets_dy[val_idx]\n",
    "# y_val_frame_ids_fold = targets_frame_ids[val_idx]\n",
    "\n",
    "# # Validation metadata (use unscaled last positions)\n",
    "# val_ids = [ids[i] for i in val_idx]\n",
    "# val_data = pd.DataFrame(val_ids)\n",
    "# val_data['x_last'] = np.array([s[-1, 0] for s in X_val_unscaled])\n",
    "# val_data['y_last'] = np.array([s[-1, 1] for s in X_val_unscaled])\n",
    "\n",
    "# # Fit scaler on training-fold frames only (no leakage)\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(np.vstack([s for s in X_train_unscaled]))\n",
    "\n",
    "# def apply_scaler_to_sequences(seq_array, scaler):\n",
    "#     return np.array([scaler.transform(s) for s in seq_array], dtype=object)\n",
    "\n",
    "# X_train_fold = apply_scaler_to_sequences(X_train_unscaled, scaler)\n",
    "# X_val_fold = apply_scaler_to_sequences(X_val_unscaled, scaler)\n",
    "\n",
    "# input_dim = X_train_unscaled[0].shape[-1]\n",
    "# H = Config.MAX_FUTURE_HORIZON\n",
    "# # # Default block specs if not defined earlier\n",
    "# # if 'block_specs' not in globals():\n",
    "# def expand_block_specs(specs):\n",
    "#     out = []\n",
    "#     for spec in specs:\n",
    "#         k = int(spec.get(\"repeat\", 1))\n",
    "#         spec = {k_: v for k_, v in spec.items() if k_ != \"repeat\"}\n",
    "#         out.extend([dict(spec) for _ in range(k)])\n",
    "#     return out\n",
    "# base_specs = [\n",
    "#     {\"type\": \"rnn\", \"rnn\": \"gru\", \"hidden\": 128, \"layers\": 1, \"dropout\": 0.1, \"repeat\": 1},\n",
    "#     {\"type\": \"transformer\", \"nhead\": 4, \"ff_mult\": 4, \"dropout\": 0.1, \"repeat\": 1},\n",
    "#     {\"type\": \"tcn\", \"kernel\": 3, \"dilation\": 2, \"dropout\": 0.1, \"repeat\": 1},\n",
    "# ]\n",
    "# block_specs = expand_block_specs(base_specs)\n",
    "# # Train separate axis models\n",
    "\n",
    "# print(\"\\nTraining axis X model (first fold)...\")\n",
    "# model_x, best_loss_x = train_axis_model(\n",
    "#     X_train_fold, y_train_dx_fold, X_val_fold, y_val_dx_fold,\n",
    "#     input_dim=input_dim, horizon=H, block_specs=block_specs,\n",
    "#     pooling=\"mean\", predict_mode=\"steps\",\n",
    "#     batch_size=Config.BATCH_SIZE, epochs=200, lr=Config.LEARNING_RATE,\n",
    "#     patience=Config.PATIENCE, delta=0.5, time_decay=0.03, verbose_every=5\n",
    "# )\n",
    "\n",
    "# print(\"\\nTraining axis Y model (first fold)...\")\n",
    "# model_y, best_loss_y = train_axis_model(\n",
    "#     X_train_fold, y_train_dy_fold, X_val_fold, y_val_dy_fold,\n",
    "#     input_dim=input_dim, horizon=H, block_specs=block_specs,\n",
    "#     pooling=\"mean\", predict_mode=\"steps\",\n",
    "#     batch_size=Config.BATCH_SIZE, epochs=200, lr=Config.LEARNING_RATE,\n",
    "#     patience=Config.PATIENCE, delta=0.5, time_decay=0.03, verbose_every=5\n",
    "# )\n",
    "\n",
    "# # Save fold_1 artifacts\n",
    "# os.makedirs('fold_1', exist_ok=True)\n",
    "# torch.save({'state_dict': model_x.state_dict(), 'config': {'input_dim': input_dim, 'horizon': H}}, 'fold_1/axis_x.pt')\n",
    "# torch.save({'state_dict': model_y.state_dict(), 'config': {'input_dim': input_dim, 'horizon': H}}, 'fold_1/axis_y.pt')\n",
    "# joblib.dump(scaler, 'fold_1/lstm_feature_scaler_fold.joblib')\n",
    "\n",
    "# # OOF predictions and score for this fold\n",
    "# oof_pred_1, oof_true_1 = create_oof_predictions_xy(\n",
    "#     model_x, model_y, scaler,\n",
    "#     X_val_unscaled, val_ids,\n",
    "#     y_val_dx_fold, y_val_dy_fold, y_val_frame_ids_fold,\n",
    "#     val_data, horizon=H\n",
    "# )\n",
    "# fold1_rmse = score(oof_true_1, oof_pred_1, 'id')\n",
    "# print(f\"\\n[Fold 1] RMSE: {fold1_rmse:.5f}\")\n",
    "\n",
    "# # Expose as lists for downstream inference utilities\n",
    "# models_x = [model_x]\n",
    "# models_y = [model_y]\n",
    "# scalers = [scaler]\n",
    "# # ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f91e459",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:32.698106Z",
     "iopub.status.busy": "2025-10-08T15:39:32.697711Z",
     "iopub.status.idle": "2025-10-08T15:39:32.700732Z",
     "shell.execute_reply": "2025-10-08T15:39:32.700189Z"
    },
    "papermill": {
     "duration": 0.013217,
     "end_time": "2025-10-08T15:39:32.701668",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.688451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# oof_true_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0255ee3",
   "metadata": {
    "papermill": {
     "duration": 0.008338,
     "end_time": "2025-10-08T15:39:32.718622",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.710284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65010c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:32.736406Z",
     "iopub.status.busy": "2025-10-08T15:39:32.736224Z",
     "iopub.status.idle": "2025-10-08T15:39:32.742697Z",
     "shell.execute_reply": "2025-10-08T15:39:32.742217Z"
    },
    "papermill": {
     "duration": 0.016613,
     "end_time": "2025-10-08T15:39:32.743749",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.727136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_with_improved_lstm(model, X_test, test_data,test_template=None, return_all=True):\n",
    "    \"\"\"\n",
    "    Predict cumulative displacements for each horizon.\n",
    "    Returns:\n",
    "      pred_first_x, pred_first_y, dx_cum, dy_cum, (optional) abs_all_x, abs_all_y\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    X = np.array(X_test, dtype=np.float32)\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X))\n",
    "    loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "    dx_list, dy_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for (batch,) in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)  # (B, H, 2) cumulative displacements\n",
    "            # print(f\"Predicted batch shape: {out.shape}\")\n",
    "            dx_list.append(out[:, :, 0].cpu().numpy())\n",
    "            dy_list.append(out[:, :, 1].cpu().numpy())\n",
    "    # print(f\"Predicted {len(dx_list)} batches\")\n",
    "    if not dx_list:\n",
    "        print(\"WARNING: No predictions made. Using fallback.\")\n",
    "        empty = np.zeros((0, getattr(model, \"max_frames_output\", 1)))\n",
    "        return empty, empty, empty, empty, empty, empty\n",
    "    dx_cum = np.vstack(dx_list)\n",
    "    dy_cum = np.vstack(dy_list)\n",
    "    x_last = test_data['x_last'].values\n",
    "    y_last = test_data['y_last'].values\n",
    "    abs_all_x = x_last[:, None] + dx_cum\n",
    "    abs_all_y = y_last[:, None] + dy_cum\n",
    "    abs_all_x = np.clip(abs_all_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "    abs_all_y = np.clip(abs_all_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "    pred_first_x = abs_all_x[:, 0]\n",
    "    pred_first_y = abs_all_y[:, 0]\n",
    "    # print(pred_first_x.shape, pred_first_y.shape, dx_cum.shape, dy_cum.shape, abs_all_x.shape, abs_all_y.shape)\n",
    "    # print(abs_all_x[0])\n",
    "    if return_all:\n",
    "        return pred_first_x, pred_first_y, dx_cum, dy_cum, abs_all_x, abs_all_y\n",
    "    # print(pred_first_x.shape, pred_first_y.shape, dx_cum.shape, dy_cum.shape)\n",
    "    return pred_first_x, pred_first_y, dx_cum, dy_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd0b9434",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:32.761796Z",
     "iopub.status.busy": "2025-10-08T15:39:32.761586Z",
     "iopub.status.idle": "2025-10-08T15:39:32.771993Z",
     "shell.execute_reply": "2025-10-08T15:39:32.771421Z"
    },
    "papermill": {
     "duration": 0.020522,
     "end_time": "2025-10-08T15:39:32.772955",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.752433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_ensemble_predictions_xy(\n",
    "    models_x, models_y, scalers, X_test_unscaled, test_seq_ids, test_template, batch_size=1024\n",
    "):\n",
    "    \"\"\"\n",
    "    Ensemble test-time predictions using separate axis models (dx and dy) across folds.\n",
    "    - models_x, models_y: lists of FlexibleSeqModel (same length, one per fold)\n",
    "    - scalers: list of StandardScaler, aligned with models (or None entries)\n",
    "    - X_test_unscaled: list/array of (T,F) sequences (unscaled)\n",
    "    - test_seq_ids: list of dicts with keys [game_id, play_id, nfl_id, frame_id(last)]\n",
    "    - test_template: DataFrame with required submission rows\n",
    "\n",
    "    Returns: DataFrame with columns [id, x, y]\n",
    "    \"\"\"\n",
    "    if len(models_x) == 0 or len(models_x) != len(models_y):\n",
    "        print(\"No axis models or mismatched model counts.\")\n",
    "        return None\n",
    "    if scalers is not None and len(scalers) != len(models_x):\n",
    "        raise ValueError(\"Length of scalers must match number of folds (or be None).\")\n",
    "\n",
    "    # Convert sequences to array of objects for robust handling\n",
    "    X_test_unscaled = np.array(X_test_unscaled, dtype=object)\n",
    "    N = len(X_test_unscaled)\n",
    "\n",
    "    # Last observed absolute positions from the sequences (assumes feat[0]=x, feat[1]=y)\n",
    "    x_last = np.array([seq[-1, 0] for seq in X_test_unscaled], dtype=np.float32)\n",
    "    y_last = np.array([seq[-1, 1] for seq in X_test_unscaled], dtype=np.float32)\n",
    "\n",
    "    # Per-fold cumulative displacement predictions\n",
    "    per_fold_dx = []\n",
    "    per_fold_dy = []\n",
    "\n",
    "    for i in range(len(models_x)):\n",
    "        model_x = models_x[i]\n",
    "        model_y = models_y[i]\n",
    "        scaler = scalers[i] if scalers is not None else None\n",
    "\n",
    "        # Scale per sequence for this fold\n",
    "        if scaler is not None:\n",
    "            scaled = np.array([scaler.transform(s) for s in X_test_unscaled], dtype=object)\n",
    "        else:\n",
    "            scaled = X_test_unscaled\n",
    "\n",
    "        # Stack to (N,T,F)\n",
    "        X = np.stack(scaled.astype(np.float32))\n",
    "        device = next(model_x.parameters()).device\n",
    "        ds = TensorDataset(torch.from_numpy(X))\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        dx_list, dy_list = [], []\n",
    "        model_x.eval(); model_y.eval()\n",
    "        with torch.no_grad():\n",
    "            for (batch,) in dl:\n",
    "                batch = batch.to(device)    # (B,T,F)\n",
    "                dx = model_x(batch)         # (B,H)\n",
    "                dy = model_y(batch)         # (B,H)\n",
    "                dx_list.append(dx.cpu().numpy())\n",
    "                dy_list.append(dy.cpu().numpy())\n",
    "        dx_cum = np.vstack(dx_list)  # (N,H)\n",
    "        dy_cum = np.vstack(dy_list)  # (N,H)\n",
    "\n",
    "        per_fold_dx.append(dx_cum)\n",
    "        per_fold_dy.append(dy_cum)\n",
    "\n",
    "    # Ensemble by mean across folds\n",
    "    ens_dx = np.mean(np.stack(per_fold_dx, axis=0), axis=0)  # (N,H)\n",
    "    ens_dy = np.mean(np.stack(per_fold_dy, axis=0), axis=0)  # (N,H)\n",
    "\n",
    "    # Create submission rows by mapping to test_template frame order per (game,play,nfl)\n",
    "    test_meta = pd.DataFrame(test_seq_ids)\n",
    "    out_rows = []\n",
    "    H = ens_dx.shape[1]\n",
    "    for i, seq_info in test_meta.iterrows():\n",
    "        game_id = int(seq_info['game_id'])\n",
    "        play_id = int(seq_info['play_id'])\n",
    "        nfl_id = int(seq_info['nfl_id'])\n",
    "\n",
    "        frame_ids = (\n",
    "            test_template[\n",
    "                (test_template['game_id'] == game_id) &\n",
    "                (test_template['play_id'] == play_id) &\n",
    "                (test_template['nfl_id'] == nfl_id)\n",
    "            ]['frame_id'].sort_values().tolist()\n",
    "        )\n",
    "        for t, frame_id in enumerate(frame_ids):\n",
    "            tt = t if t < H else H - 1\n",
    "            px = np.clip(x_last[i] + ens_dx[i, tt], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "            py = np.clip(y_last[i] + ens_dy[i, tt], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            out_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_id}\",\n",
    "                'x': px,\n",
    "                'y': py\n",
    "            })\n",
    "    submission = pd.DataFrame(out_rows)\n",
    "    return submission\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "345fb3f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:32.792637Z",
     "iopub.status.busy": "2025-10-08T15:39:32.792421Z",
     "iopub.status.idle": "2025-10-08T15:39:32.800349Z",
     "shell.execute_reply": "2025-10-08T15:39:32.799790Z"
    },
    "papermill": {
     "duration": 0.0181,
     "end_time": "2025-10-08T15:39:32.801330",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.783230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_ensemble_val_predictions(models, scalers, X_val_unscaled, val_ids, y_val_dx_fold, y_val_dy_fold, val_data, exclude_fold=None):\n",
    "    \"\"\"\n",
    "    Generate ensemble predictions for validation data and prepare for scoring.\n",
    "    Excludes the model from the same fold to prevent potential overfitting/leakage.\n",
    "    \n",
    "    Args:\n",
    "        models: List of trained models\n",
    "        scalers: List of scalers (one per model)\n",
    "        X_val_unscaled: Validation sequences (unscaled)\n",
    "        val_ids: List of dicts with sequence metadata\n",
    "        y_val_dx_fold, y_val_dy_fold: Ground truth displacements\n",
    "        val_data: DataFrame with x_last, y_last\n",
    "        exclude_fold: Index of the fold to exclude (0-based)\n",
    "    \n",
    "    Returns:\n",
    "        ensemble_pred_df, ensemble_true_df: DataFrames for scoring\n",
    "    \"\"\"\n",
    "    pred_rows = []\n",
    "    true_rows = []\n",
    "    \n",
    "    for i, seq_info in enumerate(val_ids):\n",
    "        game_id = seq_info['game_id']\n",
    "        play_id = seq_info['play_id']\n",
    "        nfl_id = seq_info['nfl_id']\n",
    "        x_last = val_data.iloc[i]['x_last']\n",
    "        y_last = val_data.iloc[i]['y_last']\n",
    "        \n",
    "        # Ground truth\n",
    "        dx_true = y_val_dx_fold[i]\n",
    "        dy_true = y_val_dy_fold[i]\n",
    "        for t in range(len(dx_true)):\n",
    "            frame_rel = t + 1\n",
    "            true_x = x_last + dx_true[t]\n",
    "            true_y = y_last + dy_true[t]\n",
    "            true_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{frame_rel}\",\n",
    "                'x': true_x,\n",
    "                'y': true_y\n",
    "            })\n",
    "        \n",
    "        # Ensemble predictions (exclude the model from the same fold)\n",
    "        per_model_dx = []\n",
    "        per_model_dy = []\n",
    "        for j, model in enumerate(models):\n",
    "            if exclude_fold is not None and j == exclude_fold:\n",
    "                continue  # Skip the model trained on this fold\n",
    "            scaler = scalers[j]\n",
    "            scaled_seq = scaler.transform(X_val_unscaled[i]).astype(np.float32)\n",
    "            scaled_seq = torch.tensor(scaled_seq).unsqueeze(0).to(next(model.parameters()).device)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(scaled_seq).cpu().numpy()[0]  # (max_frames_output, 2)\n",
    "            per_model_dx.append(output[:, 0])\n",
    "            per_model_dy.append(output[:, 1])\n",
    "        \n",
    "        # Average across remaining models\n",
    "        if per_model_dx:  # Ensure there are models to average\n",
    "            ens_dx = np.mean(per_model_dx, axis=0)\n",
    "            ens_dy = np.mean(per_model_dy, axis=0)\n",
    "        else:\n",
    "            # Fallback: use the last known position (though this shouldn't happen with n_folds > 1)\n",
    "            ens_dx = np.zeros(len(dx_true))\n",
    "            ens_dy = np.zeros(len(dy_true))\n",
    "        \n",
    "        # Generate predictions for each frame\n",
    "        for t in range(len(dx_true)):\n",
    "            pred_x = x_last + ens_dx[t]\n",
    "            pred_y = y_last + ens_dy[t]\n",
    "            pred_rows.append({\n",
    "                'id': f\"{game_id}_{play_id}_{nfl_id}_{t+1}\",\n",
    "                'x': np.clip(pred_x, Config.FIELD_X_MIN, Config.FIELD_X_MAX),\n",
    "                'y': np.clip(pred_y, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(pred_rows), pd.DataFrame(true_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb60832",
   "metadata": {
    "papermill": {
     "duration": 0.008475,
     "end_time": "2025-10-08T15:39:32.818268",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.809793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5folds training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961daeca",
   "metadata": {
    "papermill": {
     "duration": 0.008414,
     "end_time": "2025-10-08T15:39:32.835145",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.826731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d3319a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:32.853113Z",
     "iopub.status.busy": "2025-10-08T15:39:32.852627Z",
     "iopub.status.idle": "2025-10-08T15:39:33.690094Z",
     "shell.execute_reply": "2025-10-08T15:39:33.689343Z"
    },
    "papermill": {
     "duration": 0.847687,
     "end_time": "2025-10-08T15:39:33.691318",
     "exception": false,
     "start_time": "2025-10-08T15:39:32.843631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences with NaN: 0\n"
     ]
    }
   ],
   "source": [
    "# Check NaN in sequences robustly\n",
    "nan_count = 0\n",
    "for i, seq in enumerate(sequences):\n",
    "    try:\n",
    "        arr = np.array(seq, dtype=np.float32)\n",
    "        if np.isnan(arr).any():\n",
    "            nan_mask = np.isnan(arr)\n",
    "            nan_features = np.where(nan_mask.any(axis=0))[0]\n",
    "            print(f\"WARNING: NaN values found in sequence index {i}, feature columns: {nan_features}\")\n",
    "            nan_count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Could not check sequence {i}: {e}\")\n",
    "print(f\"Total sequences with NaN: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55d24175",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:33.710170Z",
     "iopub.status.busy": "2025-10-08T15:39:33.709683Z",
     "iopub.status.idle": "2025-10-08T15:39:33.714087Z",
     "shell.execute_reply": "2025-10-08T15:39:33.713385Z"
    },
    "papermill": {
     "duration": 0.014606,
     "end_time": "2025-10-08T15:39:33.715146",
     "exception": false,
     "start_time": "2025-10-08T15:39:33.700540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def expand_block_specs(specs):\n",
    "    \"\"\"\n",
    "    Supports {\"type\": \"...\", ..., \"repeat\": k} to replicate blocks.\n",
    "    Returns a flat list of block specs (without 'repeat').\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for spec in specs:\n",
    "        k = int(spec.get(\"repeat\", 1))\n",
    "        spec = {k_: v for k_, v in spec.items() if k_ != \"repeat\"}\n",
    "        out.extend([dict(spec) for _ in range(k)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcc86901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:33.733092Z",
     "iopub.status.busy": "2025-10-08T15:39:33.732874Z",
     "iopub.status.idle": "2025-10-08T15:39:33.736877Z",
     "shell.execute_reply": "2025-10-08T15:39:33.736346Z"
    },
    "papermill": {
     "duration": 0.014231,
     "end_time": "2025-10-08T15:39:33.737916",
     "exception": false,
     "start_time": "2025-10-08T15:39:33.723685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example block configuration\n",
    "\n",
    "block_specs = expand_block_specs(Config.BASED_SPECS)\n",
    "if Config.NN_PRETRAIN_DIR is None:\n",
    "    models_x, models_y, scalers, fold_metrics, cv, oof_pred_df = run_multi_fold_training_xy(\n",
    "        sequences, targets_dx, targets_dy, targets_frame_ids, ids,\n",
    "        block_specs=block_specs,\n",
    "        pooling=\"mean\",            # simpler, robust\n",
    "        predict_mode=\"steps\",\n",
    "        lr=Config.LEARNING_RATE,\n",
    "        # n_folds = 5,\n",
    "        n_folds=Config.N_FOLDS,\n",
    "        epochs=Config.EPOCHS,\n",
    "        # epochs=10,\n",
    "        patience=Config.PATIENCE\n",
    "    )\n",
    "    print(\"Final OOF CV:\", cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b602425",
   "metadata": {
    "papermill": {
     "duration": 0.008282,
     "end_time": "2025-10-08T15:39:33.754746",
     "exception": false,
     "start_time": "2025-10-08T15:39:33.746464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168e6cd7",
   "metadata": {
    "papermill": {
     "duration": 0.0083,
     "end_time": "2025-10-08T15:39:33.771536",
     "exception": false,
     "start_time": "2025-10-08T15:39:33.763236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a36f99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:33.791887Z",
     "iopub.status.busy": "2025-10-08T15:39:33.791691Z",
     "iopub.status.idle": "2025-10-08T15:39:33.798780Z",
     "shell.execute_reply": "2025-10-08T15:39:33.798287Z"
    },
    "papermill": {
     "duration": 0.019233,
     "end_time": "2025-10-08T15:39:33.799751",
     "exception": false,
     "start_time": "2025-10-08T15:39:33.780518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_axis_model_from_config(cfg):\n",
    "    return FlexibleSeqModel(\n",
    "        input_dim=cfg['input_dim'],\n",
    "        horizon=cfg['horizon'],\n",
    "        block_specs=cfg['block_specs'],\n",
    "        dropout=cfg.get('dropout', 0.2),\n",
    "        pooling=cfg.get('pooling', 'attn'),\n",
    "        predict_mode=cfg.get('predict_mode', 'steps'),\n",
    "        attn_pool_heads=cfg.get('attn_pool_heads', 4),\n",
    "    )\n",
    "\n",
    "def save_axis_checkpoint(model, cfg, fold_dir, axis_name='x'):\n",
    "    path = Path(fold_dir) / f'axis_{axis_name}.pt'\n",
    "    torch.save({'state_dict': model.state_dict(), 'config': cfg}, str(path))\n",
    "\n",
    "def load_axis_checkpoint(fold_dir, axis_name='x', device=None):\n",
    "    device = device or Config.DEVICE\n",
    "    ckpt = torch.load(str(Path(fold_dir) / f'axis_{axis_name}.pt'), map_location=device)\n",
    "    cfg = ckpt['config']\n",
    "    model = build_axis_model_from_config(cfg).to(device)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    model.eval()\n",
    "    return model, cfg\n",
    "\n",
    "def load_folds_xy(num_folds, models_dir=None, device=None):\n",
    "    device = device or Config.DEVICE\n",
    "    base = Path(models_dir) if models_dir else Path('.')\n",
    "    models_x, models_y, scalers, cfgs = [], [], [], []\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        fold_dir = base / f'fold_{fold}'\n",
    "        try:\n",
    "            mx, cfg = load_axis_checkpoint(fold_dir, 'x', device=device)\n",
    "            my, _   = load_axis_checkpoint(fold_dir, 'y', device=device)\n",
    "            scaler = joblib.load(str(fold_dir / 'lstm_feature_scaler_fold.joblib'))\n",
    "            models_x.append(mx); models_y.append(my); scalers.append(scaler); cfgs.append(cfg)\n",
    "            print(f'Loaded fold {fold} OK')\n",
    "        except Exception as e:\n",
    "            print(f'Fold {fold} load failed: {e}')\n",
    "    return models_x, models_y, scalers, cfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "808b7409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:33.817915Z",
     "iopub.status.busy": "2025-10-08T15:39:33.817520Z",
     "iopub.status.idle": "2025-10-08T15:39:35.069963Z",
     "shell.execute_reply": "2025-10-08T15:39:35.069116Z"
    },
    "papermill": {
     "duration": 1.262894,
     "end_time": "2025-10-08T15:39:35.071253",
     "exception": false,
     "start_time": "2025-10-08T15:39:33.808359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained models from /kaggle/input/nfl-big-data-bowl-2026-public/results\n",
      "Loaded fold 1 OK\n",
      "Loaded fold 2 OK\n",
      "Loaded fold 3 OK\n",
      "Loaded fold 4 OK\n",
      "Loaded fold 5 OK\n"
     ]
    }
   ],
   "source": [
    "if Config.NN_PRETRAIN_DIR is not None:\n",
    "    print(f\"Loading pretrained models from {Config.NN_PRETRAIN_DIR}\")\n",
    "    models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=Config.NN_PRETRAIN_DIR, device=Config.DEVICE)\n",
    "else:\n",
    "    models_x_nn, models_y_nn, scalers, cfgs = load_folds_xy(num_folds=Config.N_FOLDS, models_dir=None, device=Config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61e351b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:35.090713Z",
     "iopub.status.busy": "2025-10-08T15:39:35.090160Z",
     "iopub.status.idle": "2025-10-08T15:39:47.312501Z",
     "shell.execute_reply": "2025-10-08T15:39:47.311622Z"
    },
    "papermill": {
     "duration": 12.232934,
     "end_time": "2025-10-08T15:39:47.313606",
     "exception": false,
     "start_time": "2025-10-08T15:39:35.080672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing sequences for LSTM...\n",
      "Using window size =  8\n",
      "Calculating interaction features...\n",
      "Using 63 features for LSTM input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 472/472 [00:05<00:00, 88.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 472 test sequences with shape: (8, 63).\n",
      "Saved submission_xy.csv\n"
     ]
    }
   ],
   "source": [
    "# Build test sequences\n",
    "test_sequences, test_seq_ids = prepare_sequences(\n",
    "    test_input, test_template=test_template, is_training=False, window_size=Config.WINDOW_SIZE\n",
    ")\n",
    "print(f\"Prepared {len(test_sequences)} test sequences with shape: {test_sequences[0].shape}.\")\n",
    "# Use the trained per-fold axis models\n",
    "submission_xy = create_ensemble_predictions_xy(\n",
    "    models_x=models_x_nn,\n",
    "    models_y=models_y_nn,\n",
    "    scalers=scalers,\n",
    "    X_test_unscaled=test_sequences,\n",
    "    test_seq_ids=test_seq_ids,\n",
    "    test_template=test_template,\n",
    "    batch_size=1024\n",
    ")\n",
    "submission_xy.to_csv('submission_xy.csv', index=False)\n",
    "print(\"Saved submission_xy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "378b4a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:47.337092Z",
     "iopub.status.busy": "2025-10-08T15:39:47.336651Z",
     "iopub.status.idle": "2025-10-08T15:39:47.353911Z",
     "shell.execute_reply": "2025-10-08T15:39:47.353354Z"
    },
    "papermill": {
     "duration": 0.029789,
     "end_time": "2025-10-08T15:39:47.354929",
     "exception": false,
     "start_time": "2025-10-08T15:39:47.325140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024120805_74_54586_1</td>\n",
       "      <td>88.354332</td>\n",
       "      <td>34.315254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024120805_74_54586_2</td>\n",
       "      <td>88.671532</td>\n",
       "      <td>34.352814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024120805_74_54586_3</td>\n",
       "      <td>89.009758</td>\n",
       "      <td>34.438297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024120805_74_54586_4</td>\n",
       "      <td>89.366631</td>\n",
       "      <td>34.571102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024120805_74_54586_5</td>\n",
       "      <td>89.731064</td>\n",
       "      <td>34.755199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>2025010515_3902_55112_26</td>\n",
       "      <td>100.366791</td>\n",
       "      <td>27.829220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>2025010515_3902_55112_27</td>\n",
       "      <td>101.119644</td>\n",
       "      <td>28.206562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>2025010515_3902_55112_28</td>\n",
       "      <td>101.761513</td>\n",
       "      <td>28.559616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>2025010515_3902_55112_29</td>\n",
       "      <td>102.391891</td>\n",
       "      <td>28.784012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>2025010515_3902_55112_30</td>\n",
       "      <td>103.017998</td>\n",
       "      <td>28.890644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5837 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id           x          y\n",
       "0        2024120805_74_54586_1   88.354332  34.315254\n",
       "1        2024120805_74_54586_2   88.671532  34.352814\n",
       "2        2024120805_74_54586_3   89.009758  34.438297\n",
       "3        2024120805_74_54586_4   89.366631  34.571102\n",
       "4        2024120805_74_54586_5   89.731064  34.755199\n",
       "...                        ...         ...        ...\n",
       "5832  2025010515_3902_55112_26  100.366791  27.829220\n",
       "5833  2025010515_3902_55112_27  101.119644  28.206562\n",
       "5834  2025010515_3902_55112_28  101.761513  28.559616\n",
       "5835  2025010515_3902_55112_29  102.391891  28.784012\n",
       "5836  2025010515_3902_55112_30  103.017998  28.890644\n",
       "\n",
       "[5837 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721ca04",
   "metadata": {
    "papermill": {
     "duration": 0.010838,
     "end_time": "2025-10-08T15:39:47.376817",
     "exception": false,
     "start_time": "2025-10-08T15:39:47.365979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7009fd27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:47.400099Z",
     "iopub.status.busy": "2025-10-08T15:39:47.399796Z",
     "iopub.status.idle": "2025-10-08T15:39:52.860806Z",
     "shell.execute_reply": "2025-10-08T15:39:52.860063Z"
    },
    "papermill": {
     "duration": 5.47436,
     "end_time": "2025-10-08T15:39:52.862181",
     "exception": false,
     "start_time": "2025-10-08T15:39:47.387821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CatBoost fold models: 5 X, 5 Y\n",
      "Feature count: 102\n",
      "CV RMSE per fold (abs with baseline): [0.6535194734898215, 0.6815433517961675, 0.7728707286456312, 0.6591622316603506, 0.6615027523088757]\n",
      "Engineering test features (geometry + lags)…\n",
      "Computing neighbor embeddings (last-frame, GNN-lite)…\n",
      "Predicting with CatBoost fold models (residual -> absolute)…\n",
      "Saved submission_catboost.csv\n"
     ]
    }
   ],
   "source": [
    "# === CatBoost inference: load pretrained models, build test features, predict ===\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# --------------------------- GNN-lite + FE (define if missing) --------------------------- #\n",
    "def _to_inches(h):\n",
    "    try:\n",
    "        a, b = str(h).split(\"-\")\n",
    "        return float(a) * 12.0 + float(b)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "if 'engineer_advanced_features' not in globals():\n",
    "    def engineer_advanced_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        df[\"height_inches\"] = df[\"player_height\"].map(_to_inches)\n",
    "        df[\"bmi\"] = (df[\"player_weight\"] / (df[\"height_inches\"]**2)) * 703.0\n",
    "\n",
    "        dir_rad = np.radians(df[\"dir\"].fillna(0.0))\n",
    "        df[\"heading_x\"] = np.sin(dir_rad)\n",
    "        df[\"heading_y\"] = np.cos(dir_rad)\n",
    "\n",
    "        s = df[\"s\"].fillna(0.0)\n",
    "        a = df[\"a\"].fillna(0.0)\n",
    "        df[\"velocity_x\"] = s * df[\"heading_x\"]\n",
    "        df[\"velocity_y\"] = s * df[\"heading_y\"]\n",
    "        df[\"acceleration_x\"] = a * df[\"heading_x\"]\n",
    "        df[\"acceleration_y\"] = a * df[\"heading_y\"]\n",
    "\n",
    "        dx = df[\"ball_land_x\"] - df[\"x\"]\n",
    "        dy = df[\"ball_land_y\"] - df[\"y\"]\n",
    "        dist = np.sqrt(dx**2 + dy**2)\n",
    "        df[\"dist_to_ball\"] = dist\n",
    "        df[\"angle_to_ball\"] = np.arctan2(dy, dx)\n",
    "        bux = dx / (dist + 1e-6)\n",
    "        buy = dy / (dist + 1e-6)\n",
    "\n",
    "        df[\"velocity_toward_ball\"] = df[\"velocity_x\"]*bux + df[\"velocity_y\"]*buy\n",
    "        df[\"velocity_alignment\"]   = df[\"heading_x\"]*bux + df[\"heading_y\"]*buy\n",
    "\n",
    "        df[\"speed_squared\"]   = s**2\n",
    "        df[\"accel_magnitude\"] = np.sqrt(df[\"acceleration_x\"]**2 + df[\"acceleration_y\"]**2)\n",
    "        w = df[\"player_weight\"].fillna(0.0)\n",
    "        df[\"momentum_x\"] = w * df[\"velocity_x\"]\n",
    "        df[\"momentum_y\"] = w * df[\"velocity_y\"]\n",
    "        df[\"kinetic_energy\"] = 0.5 * w * df[\"speed_squared\"]\n",
    "\n",
    "        df[\"role_targeted_receiver\"] = (df[\"player_role\"] == \"Targeted Receiver\").astype(int)\n",
    "        df[\"role_defensive_coverage\"] = (df[\"player_role\"] == \"Defensive Coverage\").astype(int)\n",
    "        df[\"role_passer\"] = (df[\"player_role\"] == \"Passer\").astype(int)\n",
    "        df[\"side_offense\"] = (df[\"player_side\"] == \"Offense\").astype(int)\n",
    "        return df\n",
    "\n",
    "if 'add_sequence_features' not in globals():\n",
    "    def add_sequence_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.sort_values([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"])\n",
    "        gcols = [\"game_id\",\"play_id\",\"nfl_id\"]\n",
    "        for lag in [1,2,3,4,5]:\n",
    "            for c in [\"x\",\"y\",\"velocity_x\",\"velocity_y\",\"s\",\"a\"]:\n",
    "                if c in df.columns:\n",
    "                    df[f\"{c}_lag{lag}\"] = df.groupby(gcols)[c].shift(lag)\n",
    "        for win in [3,5]:\n",
    "            for c in [\"x\",\"y\",\"velocity_x\",\"velocity_y\",\"s\"]:\n",
    "                if c in df.columns:\n",
    "                    df[f\"{c}_rolling_mean_{win}\"] = (\n",
    "                        df.groupby(gcols)[c].rolling(win, min_periods=1).mean()\n",
    "                          .reset_index(level=[0,1,2], drop=True)\n",
    "                    )\n",
    "                    df[f\"{c}_rolling_std_{win}\"] = (\n",
    "                        df.groupby(gcols)[c].rolling(win, min_periods=1).std()\n",
    "                          .reset_index(level=[0,1,2], drop=True)\n",
    "                    )\n",
    "        for c in [\"velocity_x\",\"velocity_y\"]:\n",
    "            if c in df.columns:\n",
    "                df[f\"{c}_delta\"] = df.groupby(gcols)[c].diff()\n",
    "        return df\n",
    "\n",
    "# Default GNN-lite knobs if not already defined\n",
    "K_NEIGH = globals().get(\"K_NEIGH\", 6)\n",
    "RADIUS  = globals().get(\"RADIUS\", 30.0)\n",
    "TAU     = globals().get(\"TAU\", 8.0)\n",
    "\n",
    "if 'compute_neighbor_embeddings' not in globals():\n",
    "    def compute_neighbor_embeddings(input_df: pd.DataFrame,\n",
    "                                    k_neigh: int = K_NEIGH,\n",
    "                                    radius: float = RADIUS,\n",
    "                                    tau: float = TAU) -> pd.DataFrame:\n",
    "        cols_needed = [\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\",\"x\",\"y\",\"velocity_x\",\"velocity_y\",\"player_side\"]\n",
    "        src = input_df[cols_needed].copy()\n",
    "\n",
    "        last = (src.sort_values([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"])\n",
    "                   .groupby([\"game_id\",\"play_id\",\"nfl_id\"], as_index=False)\n",
    "                   .tail(1)\n",
    "                   .rename(columns={\"frame_id\":\"last_frame_id\"})\n",
    "                   .reset_index(drop=True))\n",
    "\n",
    "        tmp = last.merge(\n",
    "            src.rename(columns={\n",
    "                \"frame_id\":\"nb_frame_id\",\"nfl_id\":\"nfl_id_nb\",\n",
    "                \"x\":\"x_nb\",\"y\":\"y_nb\",\"velocity_x\":\"vx_nb\",\"velocity_y\":\"vy_nb\",\"player_side\":\"player_side_nb\"\n",
    "            }),\n",
    "            left_on=[\"game_id\",\"play_id\",\"last_frame_id\"],\n",
    "            right_on=[\"game_id\",\"play_id\",\"nb_frame_id\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n",
    "\n",
    "        tmp[\"dx\"]  = tmp[\"x_nb\"] - tmp[\"x\"]\n",
    "        tmp[\"dy\"]  = tmp[\"y_nb\"] - tmp[\"y\"]\n",
    "        tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n",
    "        tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n",
    "        tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n",
    "\n",
    "        tmp = tmp[np.isfinite(tmp[\"dist\"])]\n",
    "        tmp = tmp[tmp[\"dist\"] > 1e-6]\n",
    "        if radius is not None:\n",
    "            tmp = tmp[tmp[\"dist\"] <= radius]\n",
    "\n",
    "        tmp[\"is_ally\"] = (tmp[\"player_side_nb\"].fillna(\"\") == tmp[\"player_side\"].fillna(\"\")).astype(np.float32)\n",
    "\n",
    "        keys = [\"game_id\",\"play_id\",\"nfl_id\"]\n",
    "        tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n",
    "        if k_neigh is not None:\n",
    "            tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n",
    "\n",
    "        tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n",
    "        sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n",
    "        tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"]/sum_w, 0.0)\n",
    "\n",
    "        tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n",
    "        tmp[\"wn_opp\"]  = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n",
    "\n",
    "        for col in [\"dx\",\"dy\",\"dvx\",\"dvy\"]:\n",
    "            tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n",
    "            tmp[f\"{col}_opp_w\"]  = tmp[col] * tmp[\"wn_opp\"]\n",
    "\n",
    "        tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n",
    "        tmp[\"dist_opp\"]  = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n",
    "\n",
    "        ag = tmp.groupby(keys).agg(\n",
    "            gnn_ally_dx_mean = (\"dx_ally_w\",\"sum\"),\n",
    "            gnn_ally_dy_mean = (\"dy_ally_w\",\"sum\"),\n",
    "            gnn_ally_dvx_mean= (\"dvx_ally_w\",\"sum\"),\n",
    "            gnn_ally_dvy_mean= (\"dvy_ally_w\",\"sum\"),\n",
    "            gnn_opp_dx_mean  = (\"dx_opp_w\",\"sum\"),\n",
    "            gnn_opp_dy_mean  = (\"dy_opp_w\",\"sum\"),\n",
    "            gnn_opp_dvx_mean = (\"dvx_opp_w\",\"sum\"),\n",
    "            gnn_opp_dvy_mean = (\"dvy_opp_w\",\"sum\"),\n",
    "            gnn_ally_cnt     = (\"is_ally\",\"sum\"),\n",
    "            gnn_opp_cnt      = (\"is_ally\", lambda s: float(len(s) - s.sum())),\n",
    "            gnn_ally_dmin    = (\"dist_ally\",\"min\"),\n",
    "            gnn_ally_dmean   = (\"dist_ally\",\"mean\"),\n",
    "            gnn_opp_dmin     = (\"dist_opp\",\"min\"),\n",
    "            gnn_opp_dmean    = (\"dist_opp\",\"mean\"),\n",
    "        ).reset_index()\n",
    "\n",
    "        near = tmp.loc[tmp[\"rnk\"]<=3, keys+[\"rnk\",\"dist\"]].copy()\n",
    "        near[\"rnk\"] = near[\"rnk\"].astype(int)\n",
    "        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n",
    "        dwide = dwide.rename(columns={1:\"gnn_d1\",2:\"gnn_d2\",3:\"gnn_d3\"}).reset_index()\n",
    "        ag = ag.merge(dwide, on=keys, how=\"left\")\n",
    "\n",
    "        for c in [\"gnn_ally_dx_mean\",\"gnn_ally_dy_mean\",\"gnn_ally_dvx_mean\",\"gnn_ally_dvy_mean\",\n",
    "                  \"gnn_opp_dx_mean\",\"gnn_opp_dy_mean\",\"gnn_opp_dvx_mean\",\"gnn_opp_dvy_mean\"]:\n",
    "            ag[c] = ag[c].fillna(0.0)\n",
    "        for c in [\"gnn_ally_cnt\",\"gnn_opp_cnt\"]:\n",
    "            ag[c] = ag[c].fillna(0.0)\n",
    "        for c in [\"gnn_ally_dmin\",\"gnn_opp_dmin\",\"gnn_ally_dmean\",\"gnn_opp_dmean\",\"gnn_d1\",\"gnn_d2\",\"gnn_d3\"]:\n",
    "            ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n",
    "        return ag\n",
    "\n",
    "if 'physics_baseline' not in globals():\n",
    "    def physics_baseline(x_last, y_last, vx_last, vy_last, dt):\n",
    "        px = x_last + vx_last * dt\n",
    "        py = y_last + vy_last * dt\n",
    "        px = np.clip(px, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "        py = np.clip(py, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "        return px, py\n",
    "\n",
    "# --------------------------- Load pretrained artifacts --------------------------- #\n",
    "ckpt_name = \"catboost_models_5fold_gnnlite.pkl\"\n",
    "cat_ckpt_path = os.path.join(Config.CATBOOST_PRETRAIN_DIR, ckpt_name)\n",
    "if not os.path.exists(cat_ckpt_path):\n",
    "    raise FileNotFoundError(f\"CatBoost models not found at {cat_ckpt_path}\")\n",
    "\n",
    "with open(cat_ckpt_path, \"rb\") as f:\n",
    "    cat_art = pickle.load(f)\n",
    "\n",
    "# Baseline artifact structure: lists of models per fold, and a single feature list\n",
    "models_x_cb = cat_art[\"models_x\"]   # list[CatBoostRegressor]\n",
    "models_y_cb = cat_art[\"models_y\"]   # list[CatBoostRegressor]\n",
    "feat_cols_cat = cat_art[\"features\"]\n",
    "cv_rmse = cat_art.get(\"cv_rmse\", None)\n",
    "print(f\"Loaded CatBoost fold models: {len(models_x_cb)} X, {len(models_y_cb)} Y\")\n",
    "print(f\"Feature count: {len(feat_cols_cat)}\")\n",
    "if cv_rmse is not None:\n",
    "    print(f\"CV RMSE per fold (abs with baseline): {cv_rmse}\")\n",
    "\n",
    "# --------------------------- Build test features --------------------------- #\n",
    "te_in = test_input.copy()\n",
    "te_tpl = test_template.copy()\n",
    "\n",
    "print(\"Engineering test features (geometry + lags)…\")\n",
    "te_in = engineer_advanced_features(te_in)\n",
    "te_in = add_sequence_features(te_in)\n",
    "\n",
    "print(\"Computing neighbor embeddings (last-frame, GNN-lite)…\")\n",
    "gnn_te = compute_neighbor_embeddings(te_in, k_neigh=K_NEIGH, radius=RADIUS, tau=TAU)\n",
    "\n",
    "# Last observed frame per (game,play,nfl)\n",
    "agg_te = (\n",
    "    te_in.sort_values([\"game_id\",\"play_id\",\"nfl_id\",\"frame_id\"])\n",
    "         .groupby([\"game_id\",\"play_id\",\"nfl_id\"], as_index=False)\n",
    "         .tail(1)\n",
    "         .rename(columns={\"frame_id\":\"last_frame_id\"})\n",
    ")\n",
    "\n",
    "# Merge last observed stats + GNN features into template rows\n",
    "te = te_tpl.merge(agg_te, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n",
    "te = te.merge(gnn_te, on=[\"game_id\",\"play_id\",\"nfl_id\"], how=\"left\")\n",
    "\n",
    "# Time deltas (10 Hz)\n",
    "te[\"delta_frames\"] = (te[\"frame_id\"] - te[\"last_frame_id\"]).clip(lower=0).astype(float)\n",
    "te[\"delta_t\"] = te[\"delta_frames\"] / 10.0\n",
    "\n",
    "# Ensure all features exist\n",
    "for c in feat_cols_cat:\n",
    "    if c not in te.columns:\n",
    "        te[c] = 0.0\n",
    "\n",
    "# Clean feature matrix\n",
    "te.loc[:, feat_cols_cat] = (\n",
    "    te[feat_cols_cat].replace([np.inf, -np.inf], np.nan).fillna(0.0).to_numpy()\n",
    ")\n",
    "Xtest = te[feat_cols_cat].values.astype(np.float32)\n",
    "\n",
    "# Physics baseline at test\n",
    "tbx, tby = physics_baseline(\n",
    "    te[\"x\"].values, te[\"y\"].values,\n",
    "    te[\"velocity_x\"].values, te[\"velocity_y\"].values,\n",
    "    te[\"delta_t\"].values\n",
    ")\n",
    "\n",
    "# --------------------------- Predict residuals and add baseline --------------------------- #\n",
    "print(\"Predicting with CatBoost fold models (residual -> absolute)…\")\n",
    "pred_rx = np.mean([m.predict(Xtest) for m in models_x_cb], axis=0)\n",
    "pred_ry = np.mean([m.predict(Xtest) for m in models_y_cb], axis=0)\n",
    "pred_x_cat = np.clip(pred_rx + tbx, Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "pred_y_cat = np.clip(pred_ry + tby, Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "\n",
    "# ---- Save submission ----\n",
    "submission_catboost = pd.DataFrame({\n",
    "    \"id\": (te[\"game_id\"].astype(str) + \"_\" +\n",
    "           te[\"play_id\"].astype(str) + \"_\" +\n",
    "           te[\"nfl_id\"].astype(str) + \"_\" +\n",
    "           te[\"frame_id\"].astype(str)),\n",
    "    \"x\": pred_x_cat, \"y\": pred_y_cat\n",
    "})\n",
    "submission_catboost.to_csv(\"submission_catboost.csv\", index=False)\n",
    "print(\"Saved submission_catboost.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb814737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:52.886336Z",
     "iopub.status.busy": "2025-10-08T15:39:52.886102Z",
     "iopub.status.idle": "2025-10-08T15:39:52.894650Z",
     "shell.execute_reply": "2025-10-08T15:39:52.894044Z"
    },
    "papermill": {
     "duration": 0.021591,
     "end_time": "2025-10-08T15:39:52.895641",
     "exception": false,
     "start_time": "2025-10-08T15:39:52.874050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024120805_74_54586_1</td>\n",
       "      <td>88.273594</td>\n",
       "      <td>34.329840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024120805_74_54586_2</td>\n",
       "      <td>88.543739</td>\n",
       "      <td>34.356673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024120805_74_54586_3</td>\n",
       "      <td>88.901759</td>\n",
       "      <td>34.420393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024120805_74_54586_4</td>\n",
       "      <td>89.298874</td>\n",
       "      <td>34.571103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024120805_74_54586_5</td>\n",
       "      <td>89.671610</td>\n",
       "      <td>34.723818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>2025010515_3902_55112_26</td>\n",
       "      <td>100.281519</td>\n",
       "      <td>27.429677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5833</th>\n",
       "      <td>2025010515_3902_55112_27</td>\n",
       "      <td>100.953438</td>\n",
       "      <td>27.749391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5834</th>\n",
       "      <td>2025010515_3902_55112_28</td>\n",
       "      <td>101.672250</td>\n",
       "      <td>28.086680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>2025010515_3902_55112_29</td>\n",
       "      <td>102.443107</td>\n",
       "      <td>28.304687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5836</th>\n",
       "      <td>2025010515_3902_55112_30</td>\n",
       "      <td>103.045931</td>\n",
       "      <td>28.591220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5837 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id           x          y\n",
       "0        2024120805_74_54586_1   88.273594  34.329840\n",
       "1        2024120805_74_54586_2   88.543739  34.356673\n",
       "2        2024120805_74_54586_3   88.901759  34.420393\n",
       "3        2024120805_74_54586_4   89.298874  34.571103\n",
       "4        2024120805_74_54586_5   89.671610  34.723818\n",
       "...                        ...         ...        ...\n",
       "5832  2025010515_3902_55112_26  100.281519  27.429677\n",
       "5833  2025010515_3902_55112_27  100.953438  27.749391\n",
       "5834  2025010515_3902_55112_28  101.672250  28.086680\n",
       "5835  2025010515_3902_55112_29  102.443107  28.304687\n",
       "5836  2025010515_3902_55112_30  103.045931  28.591220\n",
       "\n",
       "[5837 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2cfa8d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:52.919699Z",
     "iopub.status.busy": "2025-10-08T15:39:52.919505Z",
     "iopub.status.idle": "2025-10-08T15:39:52.923730Z",
     "shell.execute_reply": "2025-10-08T15:39:52.923014Z"
    },
    "papermill": {
     "duration": 0.017548,
     "end_time": "2025-10-08T15:39:52.924748",
     "exception": false,
     "start_time": "2025-10-08T15:39:52.907200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5837, 5837)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count rows of each submission\n",
    "len(submission_xy), len(submission_catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bef73d07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:52.948154Z",
     "iopub.status.busy": "2025-10-08T15:39:52.947922Z",
     "iopub.status.idle": "2025-10-08T15:39:52.987640Z",
     "shell.execute_reply": "2025-10-08T15:39:52.986999Z"
    },
    "papermill": {
     "duration": 0.052575,
     "end_time": "2025-10-08T15:39:52.988714",
     "exception": false,
     "start_time": "2025-10-08T15:39:52.936139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv (NN-CB 50/50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Blend CatBoost with loaded NN submission (submission_xy) ----\n",
    "if 'submission_xy' in globals():\n",
    "    sub_nn = submission_xy.copy()\n",
    "    sub_cat = submission_catboost.copy()\n",
    "    ens = sub_nn.merge(sub_cat, on=\"id\", suffixes=(\"_nn\", \"_cat\"), how=\"inner\")\n",
    "    if len(ens) == 0:\n",
    "        print(\"WARNING: No common ids to blend. Skipping ensemble.\")\n",
    "    else:\n",
    "        W = Config.BLEND_WEIGHT  # blend weight NN\n",
    "        ens['x'] = np.clip(W*ens['x_nn'] + (1.0-W)*ens['x_cat'], Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
    "        ens['y'] = np.clip(W*ens['y_nn'] + (1.0-W)*ens['y_cat'], Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
    "        submission_ensemble = ens[['id','x','y']].copy()\n",
    "        submission_ensemble.to_csv(\"submission.csv\", index=False)\n",
    "        print(\"Saved submission.csv (NN-CB 50/50)\")\n",
    "else:\n",
    "    print(\"NN submission (submission_xy) not found in scope. Ensemble skipped.\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52311764",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:53.012285Z",
     "iopub.status.busy": "2025-10-08T15:39:53.012041Z",
     "iopub.status.idle": "2025-10-08T15:39:53.016211Z",
     "shell.execute_reply": "2025-10-08T15:39:53.015493Z"
    },
    "papermill": {
     "duration": 0.017146,
     "end_time": "2025-10-08T15:39:53.017384",
     "exception": false,
     "start_time": "2025-10-08T15:39:53.000238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5837, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c47552a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T15:39:53.041982Z",
     "iopub.status.busy": "2025-10-08T15:39:53.041507Z",
     "iopub.status.idle": "2025-10-08T15:39:53.048650Z",
     "shell.execute_reply": "2025-10-08T15:39:53.048126Z"
    },
    "papermill": {
     "duration": 0.020372,
     "end_time": "2025-10-08T15:39:53.049646",
     "exception": false,
     "start_time": "2025-10-08T15:39:53.029274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024120805_74_54586_1</td>\n",
       "      <td>88.309926</td>\n",
       "      <td>34.323276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024120805_74_54586_2</td>\n",
       "      <td>88.601246</td>\n",
       "      <td>34.354936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024120805_74_54586_3</td>\n",
       "      <td>88.950359</td>\n",
       "      <td>34.428450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024120805_74_54586_4</td>\n",
       "      <td>89.329364</td>\n",
       "      <td>34.571102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024120805_74_54586_5</td>\n",
       "      <td>89.698364</td>\n",
       "      <td>34.737940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id          x          y\n",
       "0  2024120805_74_54586_1  88.309926  34.323276\n",
       "1  2024120805_74_54586_2  88.601246  34.354936\n",
       "2  2024120805_74_54586_3  88.950359  34.428450\n",
       "3  2024120805_74_54586_4  89.329364  34.571102\n",
       "4  2024120805_74_54586_5  89.698364  34.737940"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_ensemble.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13825858,
     "isSourceIdPinned": false,
     "sourceId": 114239,
     "sourceType": "competition"
    },
    {
     "datasetId": 8431728,
     "sourceId": 13302452,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 68.676676,
   "end_time": "2025-10-08T15:39:56.124717",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-08T15:38:47.448041",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
